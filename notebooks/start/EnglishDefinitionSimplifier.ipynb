{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 20:05:55.719888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-19 20:06:08.048867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tyler/miniconda3/envs/tf/lib/\n",
      "2023-03-19 20:06:08.049183: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tyler/miniconda3/envs/tf/lib/\n",
      "2023-03-19 20:06:08.049206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "definitions = pd.read_csv('OPTED-Dictionary.csv')\n",
    "simpDef1 = pd.read_excel('ChildFriendlyDefinitions.xlsx', sheet_name='Sheet1')\n",
    "simpDef2 = pd.read_json('data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING (Everything below this only needs to be run once. A CSV will be created with the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cleanDataframe(df):\n",
    "    df_copy = df\n",
    "\n",
    "    # may need to take the '-' out\n",
    "    regex = \"\\[(.*?)\\]|[0-9!@#$%^&*\\/=+]|\\((.*?)\\)|\\{(.*?)\\}|\\<(.*?)\\>\"\n",
    "\n",
    "    df_copy = df_copy.replace(to_replace=regex, value=\"\", regex=True)  # remove illegal chars\n",
    "    \n",
    " \n",
    "    df_copy = df_copy.replace(to_replace=\"-\", value=\" - \", regex=True)\n",
    "    df_copy = df_copy.replace(to_replace=\"_\", value=\" _ \", regex=True)\n",
    "\n",
    "    df_copy = df_copy.replace(to_replace=\",\", value=\" , \", regex=True)\n",
    "    df_copy = df_copy.replace(to_replace=\"\\\"\", value=\" \\\" \", regex=True)\n",
    "    df_copy = df_copy.replace(to_replace=\"\\'\", value=\" \\' \", regex=True)\n",
    "    df_copy = df_copy.replace(to_replace=\"\\?\", value=\" ? \", regex=True)\n",
    "    df_copy = df_copy.replace(to_replace=\"\\.\", value=\" . \", regex=True)\n",
    "\n",
    "    \n",
    "\n",
    "    df_copy.word = df_copy.word.str.lower()  # lower case everything\n",
    "    df_copy.definition = df_copy.definition.str.lower()  # lower case everything\n",
    "\n",
    "    df_copy = df_copy.sort_values('word', ascending=True)\n",
    "    df_copy = df_copy.drop_duplicates(subset='word', keep='first')\n",
    "\n",
    "    return df_copy.reset_index().drop(['index'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ACTUAL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>' em</td>\n",
       "      <td>an obsolete or colloquial contraction of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>' gainst</td>\n",
       "      <td>a contraction of against .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>' mongst</td>\n",
       "      <td>see amongst .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>' neath</td>\n",
       "      <td>an abbreviation of beneath .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>' s</td>\n",
       "      <td>a contraction for is or  for has .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111558</th>\n",
       "      <td>zyophyte</td>\n",
       "      <td>any plant of a proposed class or grand divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111559</th>\n",
       "      <td>zythem</td>\n",
       "      <td>see zythum .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111560</th>\n",
       "      <td>zythepsary</td>\n",
       "      <td>a brewery .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111561</th>\n",
       "      <td>zythum</td>\n",
       "      <td>a kind of ancient malt beverage; a liquor mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111562</th>\n",
       "      <td>NaN</td>\n",
       "      <td>a game played with such pieces; pushpin .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111563 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                         definition\n",
       "0             ' em   an obsolete or colloquial contraction of the ...\n",
       "1         ' gainst                       a contraction of against .  \n",
       "2         ' mongst                                    see amongst .  \n",
       "3          ' neath                     an abbreviation of beneath .  \n",
       "4              ' s               a contraction for is or  for has .  \n",
       "...            ...                                                ...\n",
       "111558    zyophyte   any plant of a proposed class or grand divisi...\n",
       "111559      zythem                                     see zythum .  \n",
       "111560  zythepsary                                      a brewery .  \n",
       "111561      zythum   a kind of ancient malt beverage; a liquor mad...\n",
       "111562         NaN        a game played with such pieces; pushpin .  \n",
       "\n",
       "[111563 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions_filter = definitions.drop(['Count', 'POS'], axis=1)\n",
    "definitions_filter['word'] = definitions_filter['Word']\n",
    "definitions_filter['definition'] = definitions_filter['Definition']\n",
    "\n",
    "definitions_filter = definitions_filter.drop(['Word', 'Definition'], axis=1)\n",
    "definitions_filter = cleanDataframe(df=definitions_filter)\n",
    "\n",
    "regexQuote = \"^( \\\")|(\\\" )$\"\n",
    "definitions_filter = definitions_filter.replace(\n",
    "    to_replace=regexQuote, value=\"\", regex=True)\n",
    "definitions_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLIFIED DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accessible</td>\n",
       "      <td>when something is accessible it means anyone c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>you accommodate when you change something that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accomplish</td>\n",
       "      <td>if you accomplish something ,  you succeed in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>achieve</td>\n",
       "      <td>if you achieve something ,  you succeed in doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acre</td>\n",
       "      <td>an acre is a very large area of land about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>value</td>\n",
       "      <td>the value of a place or thing is how much mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>verify</td>\n",
       "      <td>if youÂ verifyÂ something ,  you make sure that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>vigilant</td>\n",
       "      <td>someone who isÂ vigilantÂ pays careful attention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>visible</td>\n",
       "      <td>when something is visible ,  you can see it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>wish</td>\n",
       "      <td>when you wish for something ,  you think about...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                         definition\n",
       "0     accessible  when something is accessible it means anyone c...\n",
       "1    accommodate  you accommodate when you change something that...\n",
       "2     accomplish  if you accomplish something ,  you succeed in ...\n",
       "3        achieve  if you achieve something ,  you succeed in doi...\n",
       "4           acre  an acre is a very large area of land about the...\n",
       "..           ...                                                ...\n",
       "162        value  the value of a place or thing is how much mone...\n",
       "163       verify  if youÂ verifyÂ something ,  you make sure that ...\n",
       "164     vigilant  someone who isÂ vigilantÂ pays careful attention...\n",
       "165      visible     when something is visible ,  you can see it . \n",
       "166         wish  when you wish for something ,  you think about...\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter simpDef1\n",
    "simpDef1_Filter = simpDef1.drop(['Exemplar'], axis=1)\n",
    "simpDef1_Filter['word'] = simpDef1_Filter['Word']\n",
    "simpDef1_Filter['definition'] = simpDef1_Filter['Child Friendly Definition']\n",
    "simpDef1_Filter = simpDef1_Filter.drop(\n",
    "    ['Word', 'Child Friendly Definition'], axis=1)\n",
    "simpDef1_Filter = cleanDataframe(simpDef1_Filter)\n",
    "simpDef1_Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>' tis</td>\n",
       "      <td>shortened form of  \" it is .  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>' twas</td>\n",
       "      <td>shortened form of  \" it was .  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-  ' s</td>\n",
       "      <td>a suffix used as a shortened form of  \" is .  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- able</td>\n",
       "      <td>a suffix that means capable or worthy of being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- al</td>\n",
       "      <td>a suffix that means  \" the action of .  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13972</th>\n",
       "      <td>zoo</td>\n",
       "      <td>a place where living animals ,  especially wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13973</th>\n",
       "      <td>zoo -</td>\n",
       "      <td>a prefix that means  \" animal .  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13974</th>\n",
       "      <td>zoology</td>\n",
       "      <td>the science and study of animals .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13975</th>\n",
       "      <td>zoom</td>\n",
       "      <td>to move quickly while making a low humming sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13976</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>a type of summer squash that is shaped like a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13977 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word                                         definition\n",
       "0         ' tis                   shortened form of  \" it is .  \" \n",
       "1        ' twas                  shortened form of  \" it was .  \" \n",
       "2        -  ' s  a suffix used as a shortened form of  \" is .  \" Â \n",
       "3        - able  a suffix that means capable or worthy of being...\n",
       "4          - al         a suffix that means  \" the action of .  \" \n",
       "...         ...                                                ...\n",
       "13972       zoo  a place where living animals ,  especially wil...\n",
       "13973    zoo -                 a prefix that means  \" animal .  \" \n",
       "13974   zoology                the science and study of animals . \n",
       "13975      zoom  to move quickly while making a low humming sou...\n",
       "13976  zucchini  a type of summer squash that is shaped like a ...\n",
       "\n",
       "[13977 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpDef2_filter = simpDef2\n",
    "simpDef2_Filter = cleanDataframe(simpDef2_filter)\n",
    "simpDef2_Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Dataset ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "      <th>simplified_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accessible</td>\n",
       "      <td>easy of access or approach; approachable; as ...</td>\n",
       "      <td>when something is accessible it means anyone c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>to bring into agreement or harmony; to reconc...</td>\n",
       "      <td>you accommodate when you change something that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accomplish</td>\n",
       "      <td>to bring to an issue of full success; to effe...</td>\n",
       "      <td>if you accomplish something ,  you succeed in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>achieve</td>\n",
       "      <td>to carry on to a final close; to bring out in...</td>\n",
       "      <td>if you achieve something ,  you succeed in doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acre</td>\n",
       "      <td>a piece of land  containing  square rods or  ...</td>\n",
       "      <td>an acre is a very large area of land about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10880</th>\n",
       "      <td>zither</td>\n",
       "      <td>an instrument of music used in austria and ge...</td>\n",
       "      <td>a stringed instrument that has a flat sound bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>zodiac</td>\n",
       "      <td>an imaginary belt in the heavens   ?  or  ?  ...</td>\n",
       "      <td>an imaginary belt in the heavens that includes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>zone</td>\n",
       "      <td>circuit; circumference .</td>\n",
       "      <td>an area that is divided from other areas becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>zoo -</td>\n",
       "      <td>a combining form from gr .  zwo n an animal a...</td>\n",
       "      <td>a prefix that means  \" animal .  \"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>zoology</td>\n",
       "      <td>that part of biology which relates to the ani...</td>\n",
       "      <td>the science and study of animals .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10885 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                         definition  \\\n",
       "0       accessible   easy of access or approach; approachable; as ...   \n",
       "1      accommodate   to bring into agreement or harmony; to reconc...   \n",
       "2       accomplish   to bring to an issue of full success; to effe...   \n",
       "3          achieve   to carry on to a final close; to bring out in...   \n",
       "4             acre   a piece of land  containing  square rods or  ...   \n",
       "...            ...                                                ...   \n",
       "10880       zither   an instrument of music used in austria and ge...   \n",
       "10881       zodiac   an imaginary belt in the heavens   ?  or  ?  ...   \n",
       "10882         zone                         circuit; circumference .     \n",
       "10883       zoo -    a combining form from gr .  zwo n an animal a...   \n",
       "10884      zoology   that part of biology which relates to the ani...   \n",
       "\n",
       "                                   simplified_definition  \n",
       "0      when something is accessible it means anyone c...  \n",
       "1      you accommodate when you change something that...  \n",
       "2      if you accomplish something ,  you succeed in ...  \n",
       "3      if you achieve something ,  you succeed in doi...  \n",
       "4      an acre is a very large area of land about the...  \n",
       "...                                                  ...  \n",
       "10880  a stringed instrument that has a flat sound bo...  \n",
       "10881  an imaginary belt in the heavens that includes...  \n",
       "10882  an area that is divided from other areas becau...  \n",
       "10883                a prefix that means  \" animal .  \"   \n",
       "10884                the science and study of animals .   \n",
       "\n",
       "[10885 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basically we are combining all the dataset together and putting it into a csv\n",
    "# Doing this because i don't want to constantly run this script (takes up RAM that I dont have)\n",
    "\n",
    "df = pd.DataFrame(columns=['word', 'definition', 'simplified_definition'])\n",
    "\n",
    "for index, row in simpDef1_Filter.iterrows():\n",
    "    defs = definitions_filter[row['word'] ==\n",
    "                              definitions_filter.word]['definition']\n",
    "    if defs.count() >= 1:\n",
    "        w = row['word']\n",
    "        simpDefs = row['definition']\n",
    "        defs = defs.values[0]\n",
    "        df = pd.concat([df, pd.DataFrame([[w, defs, simpDefs]], columns=[\n",
    "                       'word', 'definition', 'simplified_definition'])], ignore_index=True)\n",
    "\n",
    "for index, row in simpDef2_Filter.iterrows():\n",
    "    defs = definitions_filter[row['word'] ==\n",
    "                              definitions_filter.word]['definition']\n",
    "    if defs.count() >= 1:\n",
    "        w = row['word']\n",
    "        simpDefs = row['definition']\n",
    "        defs = defs.values[0]\n",
    "        df = pd.concat([df, pd.DataFrame([[w, defs, simpDefs]], columns=[\n",
    "                       'word', 'definition', 'simplified_definition'])], ignore_index=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"fullDataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "fullDs = pd.read_csv(\"fullDataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def convertTextToNumbers(words, tokenizer=None, padding=None, isTrainY=False):\n",
    "    # Create a tokenizer and fit it on the entire text corpus\n",
    "    if tokenizer == None:\n",
    "        #tokenizer = Tokenizer(split=' ')\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(words)\n",
    "\n",
    "    # Convert the text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(words)\n",
    "\n",
    "    if isTrainY:\n",
    "        tokenizer.word_index['<start>'] = len(\n",
    "            tokenizer.word_index) + 1\n",
    "        tokenizer.word_index['<end>'] = len(\n",
    "            tokenizer.word_index) + 1\n",
    "        sequences = [[tokenizer.word_index['<start>']] +\n",
    "                     seq + [tokenizer.word_index['<end>']] for seq in sequences]\n",
    "\n",
    "    # Get the word index mapping\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Making sure all inputs are of the same length\n",
    "    if padding == None:\n",
    "        sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, padding='post')\n",
    "    else:\n",
    "        sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, maxlen=padding, padding='post')\n",
    "    return (sequences, word_index, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m input_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(input_texts)\n\u001b[1;32m      9\u001b[0m max_len_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_seq)\n\u001b[0;32m---> 10\u001b[0m input_pad_seq \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m(input_seq, maxlen\u001b[38;5;241m=\u001b[39mmax_len_input, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m simp_eng_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m     13\u001b[0m simp_eng_tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(simplified_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "input_texts = ['dog is an animal', 'cat is a mammal', 'fish lives in water']\n",
    "simplified_texts = ['dog is animal', 'cat is mammal', 'fish lives in water']\n",
    "\n",
    "# Tokenize input and output data\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_seq = input_tokenizer.texts_to_sequences(input_texts)\n",
    "max_len_input = max(len(seq) for seq in input_seq)\n",
    "input_pad_seq = pad_sequences(input_seq, maxlen=max_len_input, padding='post')\n",
    "\n",
    "simp_eng_tokenizer = Tokenizer()\n",
    "simp_eng_tokenizer.fit_on_texts(simplified_texts)\n",
    "simp_eng_seq = simp_eng_tokenizer.texts_to_sequences(simplified_texts)\n",
    "max_len_output = max(len(seq) for seq in simp_eng_seq)\n",
    "simp_eng_pad_seq = pad_sequences(\n",
    "    simp_eng_seq, maxlen=max_len_output, padding='post')\n",
    "\n",
    "# Add start and end tokens to decoder input and target data\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(simplified_texts), max_len_output), dtype='int32')\n",
    "decoder_target_data = np.zeros((len(simplified_texts), max_len_output, len(\n",
    "    simp_eng_tokenizer.word_index) + 1), dtype='float32')\n",
    "\n",
    "\n",
    "for i, text in enumerate(simplified_texts):\n",
    "    tokens = ['<start>'] + text.split() + ['<end>']\n",
    "    for j, token in enumerate(tokens):\n",
    "        decoder_input_data[i, j] = simp_eng_tokenizer.word_index.get(token, 0)\n",
    "        if j > 0:\n",
    "            k = simp_eng_tokenizer.word_index.get(token, 0)\n",
    "            decoder_target_data[i, j - 1, k] = 1.0\n",
    "\n",
    "# Print example input and target data for the first sample\n",
    "print('Encoder input data:', input_pad_seq[0])\n",
    "print('Decoder input data:', decoder_input_data[0])\n",
    "print('Decoder target data:')\n",
    "for j in range(max_len_output):\n",
    "    print(simp_eng_tokenizer.index_word[np.argmax(\n",
    "        decoder_target_data[0, j])], end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set (do not need test) - we will provide a definition and evaluate it ourselves\n",
    "# we also may not need a validation set since i dont know if it will evaluate it properly.\n",
    "fullDs = fullDs.sample(frac=1)  # shuffle the dataset\n",
    "fullDs = fullDs.astype(str)\n",
    "\n",
    "# the two arrays below are tokenized and padded for the algorithm\n",
    "full_train_x, defWordIndex, defTokenizer = convertTextToNumbers(\n",
    "    fullDs['definition'].values[0:10000])\n",
    "\n",
    "full_train_y, simpDefWordIndex, simpDefTokenizer = convertTextToNumbers(\n",
    "    fullDs['simplified_definition'].values[0:10000], isTrainY=True)\n",
    "\n",
    "full_train_y_RAW = fullDs['simplified_definition'].values[0:10000]\n",
    "#decoder_target_data_y = createOneHotEncodingVector(full_train_y, len(simpDefWordIndex), simpDefWordIndex)\n",
    "#full_train_x = full_train_x.astype('int')\n",
    "#full_train_y = full_train_y.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10918"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fullDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6350, 2709, 3866,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8993,  733,   44,    3, 8994,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10918\n",
      "8000\n",
      "8000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_x = full_train_x[:int(len(full_train_x) * train_ratio)]\n",
    "train_y = full_train_y[:int(len(full_train_y) * train_ratio)]\n",
    "val_x = full_train_x[int(len(full_train_x) * train_ratio):]\n",
    "val_y = full_train_y[int(len(full_train_y) * train_ratio):]\n",
    "\n",
    "# Convert the Pandas DataFrame to TensorFlow Dataset\n",
    "#train_ds = tf.data.Dataset.from_tensor_slices((train_df.values[:, :-1], train_df.values[:, -1]))\n",
    "#val_ds = tf.data.Dataset.from_tensor_slices((val_df.values[:, :-1], val_df.values[:, -1]))\n",
    "print(len(fullDs['definition']))  # 10918\n",
    "print(len(train_x))  # 8734\n",
    "print(len(train_y))  # 8734\n",
    "print(len(val_x))  # 2184\n",
    "print(len(val_y))  # 2184\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   4  347  548  304    1 1655  851    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[8655    1   37 2165    4 1265  597 8656    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "a low flat mass of floating ice\n",
      "a large sheet of floating ice\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "      <th>simplified_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>floe</td>\n",
       "      <td>A low  flat mass of floating ice.</td>\n",
       "      <td>a large sheet of floating ice.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word                         definition           simplified_definition\n",
       "3981  floe  A low  flat mass of floating ice.  a large sheet of floating ice."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_train_x[0])  # def (should be head(1))\n",
    "print(full_train_y[0])  # simp def\n",
    "\n",
    "print(defTokenizer.sequences_to_texts([full_train_x[0]])[0])\n",
    "print(simpDefTokenizer.sequences_to_texts([full_train_y[0]])[0])\n",
    "print(\"\\n\\n\\n\")\n",
    "fullDs.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17618\n",
      "8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:53:55.222915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:53:56.130610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:53:56.131197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:53:56.132946: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 13:53:56.133856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:53:56.134376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:53:56.134700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:54:09.660285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:54:09.660698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:54:09.661019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-16 13:54:09.661264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3354 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 100)    1761800     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    865600      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 170),        184280      ['embedding[0][0]']              \n",
      "                                 (None, 170),                                                     \n",
      "                                 (None, 170)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 170),  184280      ['embedding_1[0][0]',            \n",
      "                                 (None, 170),                     'lstm[0][1]',                   \n",
      "                                 (None, 170)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 8656)  1480176     ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,476,136\n",
      "Trainable params: 4,476,136\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(encoding_token, decoding_token, embedding_dim, latent_dim):\n",
    "\n",
    "    # Define input sequence\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define encoder embedding layer\n",
    "    enc_emb = Embedding(input_dim=encoding_token,\n",
    "                        output_dim=embedding_dim)(inputs)\n",
    "\n",
    "    # Define encoder LSTM\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    # Discard encoder outputs and only keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Define decoder input sequence\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define decoder embedding layer\n",
    "    dec_emb_layer = Embedding(input_dim=decoding_token,\n",
    "                              output_dim=embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Define decoder LSTM\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Define decoder output layer\n",
    "    #flat = Flatten()(decoder_outputs)\n",
    "\n",
    "    #decoder_dense = Dense(decoding_token, activation='softmax')\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(decoding_token, activation='linear'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "encoding_token = len(defWordIndex)\n",
    "decoding_token = len(simpDefWordIndex)\n",
    "print(encoding_token)\n",
    "print(decoding_token)\n",
    "# usually 50 - 500 higher = more complex relation but higher chance of overfitting.\n",
    "embedding_dim = 100\n",
    "# usually 128 - 256 higher = more complex relation but higher chance of overfitting.\n",
    "latent_dim = 170\n",
    "model = createModel(encoding_token=encoding_token, decoding_token=decoding_token,\n",
    "                    embedding_dim=embedding_dim, latent_dim=latent_dim)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17619\n",
      "8657\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 128)    2255232     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 50)     432850      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 128),        131584      ['embedding_2[0][0]']            \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 128),  91648       ['embedding_3[0][0]',            \n",
      "                                 (None, 128),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 128)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 8657)  1116753     ['lstm_3[0][0]']                 \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,028,067\n",
      "Trainable params: 4,028,067\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# use this one\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(encoding_token, decoding_token, embedding_dim, latent_dim):\n",
    "\n",
    "    # Define input sequence\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define encoder embedding layer\n",
    "    enc_emb = Embedding(encoding_token, latent_dim, mask_zero=True)(inputs)\n",
    "\n",
    "    # Define encoder LSTM\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    # Discard encoder outputs and only keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Define decoder input sequence\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define decoder embedding layer\n",
    "    dec_emb_layer = Embedding(decoding_token,\n",
    "                              output_dim=embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Define decoder LSTM\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Define decoder output layer\n",
    "    # flat = Flatten()(decoder_outputs)\n",
    "\n",
    "    # decoder_dense = Dense(decoding_token, activation='softmax')\n",
    "\n",
    "    decoder_dense = TimeDistributed(\n",
    "        Dense(decoding_token, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "encoding_token = len(defWordIndex) + 1\n",
    "decoding_token = len(simpDefWordIndex) + 1\n",
    "print(encoding_token)\n",
    "print(decoding_token)\n",
    "# usually 50 - 500 higher = more complex relation but higher chance of overfitting.\n",
    "embedding_dim = 50\n",
    "# usually 128 - 256 higher = more complex relation but higher chance of overfitting.\n",
    "latent_dim = 128\n",
    "model = createModel(encoding_token=encoding_token, decoding_token=decoding_token,\n",
    "                    embedding_dim=embedding_dim, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "period\n",
      "between\n",
      "late\n",
      "afternoon\n",
      "and\n",
      "nightfall.Â \n",
      "to\n",
      "climb\n",
      "with\n",
      "difficulty\n",
      "or\n",
      "in\n",
      "an\n",
      "awkward\n",
      "way,\n",
      "using\n",
      "hands\n",
      "and\n",
      "feet.\n",
      "completely;\n",
      "entirely.\n",
      "loose,\n",
      "light,\n",
      "fluffy\n",
      "matter\n",
      "such\n",
      "as\n",
      "fibers\n",
      "or\n",
      "hairs.\n",
      "not\n",
      "valuable\n",
      "or\n",
      "important;\n",
      "insignificant.\n",
      "not\n",
      "containing\n",
      "anything;\n",
      "empty.Â \n",
      "to\n",
      "hold\n",
      "or\n",
      "squeeze\n",
      "with\n",
      "the\n",
      "arms\n",
      "in\n",
      "a\n",
      "loving\n",
      "way;\n",
      "embrace.Â \n",
      "soldiers\n",
      "on\n",
      "foot,\n",
      "or\n",
      "the\n",
      "branch\n",
      "of\n",
      "the\n",
      "military\n",
      "to\n",
      "which\n",
      "they\n",
      "belong.\n",
      "being\n",
      "the\n",
      "place\n",
      "of\n",
      "birth\n",
      "or\n",
      "origin.Â \n",
      "the\n",
      "position\n",
      "between\n",
      "second\n",
      "and\n",
      "third\n",
      "base\n",
      "in\n",
      "baseball\n",
      "or\n",
      "softball,\n",
      "or\n",
      "the\n",
      "player\n",
      "in\n",
      "this\n",
      "position.\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(full_train_y_RAW[0:10]):\n",
    "    tokens = sentence.split(' ')\n",
    "    for j, token in enumerate(tokens):\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 14:27:53.318053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:27:53.326475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:27:53.326716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(input_sequences, target_sequences, target_RAW, batch_size, simpWordIndex, lengthOfSentence):\n",
    "    while True:\n",
    "        for i in range(0, len(target_RAW), batch_size):\n",
    "\n",
    "            decoder_target_data = np.zeros(\n",
    "                (batch_size, lengthOfSentence, len(simpWordIndex)+1), dtype='float32')\n",
    "            for index, sentence in enumerate(target_RAW[i:i+batch_size]):\n",
    "                #print(sentence)\n",
    "                tokens = sentence.split(' ')\n",
    "                for j, token in enumerate(tokens):\n",
    "                    temp = simpWordIndex.get(token, 0)\n",
    "                    if temp > 0:\n",
    "\n",
    "                        decoder_target_data[index, j, temp] = 1.0\n",
    "                        \n",
    "            yield [input_sequences[i:i+batch_size], target_sequences[i:i+batch_size]], decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:54:16.887526: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n",
      "2023-03-16 13:54:18.541721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-16 13:54:20.728827: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fd32fb2d800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-16 13:54:20.728880: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2023-03-16 13:54:20.733655: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-16 13:54:20.888490: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 72s 318ms/step - loss: 0.6144\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 45s 225ms/step - loss: 0.5416\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.5195\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.4994\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4833\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4700\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4582\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4468\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4362\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4266\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4175\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4090\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.4009\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.3938\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3862\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3791\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.3719\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3650\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.3584\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3521\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3461\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3396\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3333\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3273\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3215\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3156\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3100\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.3045\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2986\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2929\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2873\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.2817\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2760\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2707\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.2653\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2601\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 45s 227ms/step - loss: 0.2550\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2499\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2448\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2401\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2354\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2309\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2261\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2212\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2162\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2116\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 0.2074\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.2035\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1995\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1954\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1914\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1872\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1832\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1797\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1764\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1733\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1695\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1662\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1627\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1590\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1554\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1522\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1494\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1465\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1439\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1411\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1384\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1354\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1325\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1298\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1272\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1244\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1218\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1195\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1172\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1153\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1135\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1113\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1089\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1063\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.1036\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.1013\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 46s 227ms/step - loss: 0.0993\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0973\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0956\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0939\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0922\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0909\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0894\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.0876\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0854\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0835\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0818\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0801\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0784\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0767\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 46s 229ms/step - loss: 0.0753\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0740\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0723\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 46s 228ms/step - loss: 0.0709\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "steps_per_epoch = len(full_train_x) // batch_size\n",
    "\n",
    "generator = data_generator(full_train_x, full_train_y, full_train_y_RAW,\n",
    "                           batch_size, simpDefWordIndex, len(full_train_y[0]))\n",
    "history = model.fit(generator, epochs=100, batch_size=batch_size, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOneHotEncodingVector(simpDef, maxLengthOutput, wordIndex):\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(simpDef), maxLengthOutput, len(wordIndex) + 1), dtype='float32')\n",
    "    for i, word in enumerate(simpDef):\n",
    "        for j, token in enumerate(word):\n",
    "            index = wordIndex.get(token, 0)\n",
    "            if index > 0:\n",
    "                decoder_target_data[i, j-1, index] == 1.0\n",
    "    return decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simpDefWordIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at one time in the past; formerly.\\xa0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y_RAW[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = full_train_y_RAW[0].split(' ')[0]\n",
    "print(t)\n",
    "simpDefWordIndex.get(t, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "new_model = tf.keras.models.load_model(\n",
    "    './model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  589  2296 15016  2765     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "['west african ceremonial trumpet']\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[[6.6857528e-13 9.6580374e-01 1.1222263e-06 ... 3.5879500e-12\n",
      "   6.2926032e-13 5.5448408e-13]\n",
      "  [5.6776417e-10 3.8870314e-01 2.8225139e-01 ... 3.8784340e-12\n",
      "   6.0323241e-10 5.1888144e-10]\n",
      "  [7.1147122e-10 2.6227155e-01 2.9513540e-02 ... 3.9403559e-12\n",
      "   7.1891110e-10 6.7448680e-10]\n",
      "  ...\n",
      "  [3.4386527e-09 8.7100153e-07 1.3422754e-01 ... 1.4367613e-13\n",
      "   3.6590517e-09 3.5325878e-09]\n",
      "  [3.3565466e-09 8.8040497e-07 1.3796544e-01 ... 1.3818578e-13\n",
      "   3.5711649e-09 3.4487118e-09]\n",
      "  [3.2748728e-09 8.8935172e-07 1.4176220e-01 ... 1.3282532e-13\n",
      "   3.4837913e-09 3.3653604e-09]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data, tokens, tokenizer = convertTextToNumbers(\n",
    "    [\"West African ceremonial trumpet\"], defTokenizer, padding=len(full_train_x[0]))\n",
    "print(data)\n",
    "print(defTokenizer.sequences_to_texts(data))\n",
    "\n",
    "inputdata = np.array(data).reshape(1, -1)\n",
    "\n",
    "outputdata = np.zeros(shape=(1, len(full_train_y[0])))\n",
    "outputdata[0, 0] = simpDefTokenizer.word_index['<start>']\n",
    "\n",
    "\n",
    "newSentence = model.predict([inputdata, outputdata])\n",
    "print(newSentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.6857528e-13 9.6580374e-01 1.1222263e-06 ... 3.5879500e-12\n",
      "  6.2926032e-13 5.5448408e-13]\n",
      " [5.6776417e-10 3.8870314e-01 2.8225139e-01 ... 3.8784340e-12\n",
      "  6.0323241e-10 5.1888144e-10]\n",
      " [7.1147122e-10 2.6227155e-01 2.9513540e-02 ... 3.9403559e-12\n",
      "  7.1891110e-10 6.7448680e-10]\n",
      " ...\n",
      " [3.4386527e-09 8.7100153e-07 1.3422754e-01 ... 1.4367613e-13\n",
      "  3.6590517e-09 3.5325878e-09]\n",
      " [3.3565466e-09 8.8040497e-07 1.3796544e-01 ... 1.3818578e-13\n",
      "  3.5711649e-09 3.4487118e-09]\n",
      " [3.2748728e-09 8.8935172e-07 1.4176220e-01 ... 1.3282532e-13\n",
      "  3.4837913e-09 3.3653604e-09]]\n"
     ]
    }
   ],
   "source": [
    "print(newSentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convertPredictionToSentence(ds, tokenizer):\n",
    "    finalSentence = np.zeros((len(ds), len(ds[0])), dtype='int32')\n",
    "    for i, sentence in enumerate(ds):\n",
    "        for j, word in enumerate(sentence):\n",
    "            maxIndex = np.argmax(word, axis=0)\n",
    "            print(maxIndex)\n",
    "            finalSentence[i,j] = maxIndex\n",
    "\n",
    "    return tokenizer.sequences_to_texts(finalSentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "80\n",
      "1810\n",
      "47\n",
      "47\n",
      "458\n",
      "1285\n",
      "7\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a a a through sorrow can can numbers riding and or or or or or or through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through through and and and and and and and and and and and and and and and and and and and and and and and and and']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertPredictionToSentence(newSentence, simpDefTokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting/Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m val_accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a675ecb475794e98cc508edff942b4df7efb840dcd0161848ec7416e01ca0315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
