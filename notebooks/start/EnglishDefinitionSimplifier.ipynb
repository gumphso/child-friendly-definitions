{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 14:25:14.974461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.120001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.120421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.122485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.122871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.123198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.303576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.303971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.304312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.304563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2254 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# old model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Rescaling(1./255),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "definitions = pd.read_csv('OPTED-Dictionary.csv')\n",
    "simpDef1 = pd.read_excel('ChildFriendlyDefinitions.xlsx', sheet_name='Sheet1')\n",
    "simpDef2 = pd.read_json('data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING (Everything below this only needs to be run once. A CSV will be created with the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cleanDataframe(df):\n",
    "    df_copy = df\n",
    "\n",
    "    # may need to take the '-' out\n",
    "    regex = \"\\[(.*?)\\]|[0-9!@#$%^&*?\\/=+\\-]|\\((.*?)\\)|\\{(.*?)\\}|\\<(.*?)\\>\"\n",
    "\n",
    "    df_copy = df_copy.replace(\n",
    "        to_replace=regex, value=\"\", regex=True).dropna()  # remove illegal chars\n",
    "\n",
    "    df_copy.word = df_copy.word.str.lower()  # lower case everything\n",
    "\n",
    "    df_copy = df_copy.sort_values('word', ascending=True)\n",
    "    df_copy = df_copy.drop_duplicates(subset='word', keep='first')\n",
    "\n",
    "    return df_copy.reset_index().drop(['index'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ACTUAL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'em</td>\n",
       "      <td>An obsolete or colloquial contraction of the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'gainst</td>\n",
       "      <td>A contraction of Against.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'mongst</td>\n",
       "      <td>See Amongst.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'neath</td>\n",
       "      <td>An abbreviation of Beneath.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s</td>\n",
       "      <td>A contraction for is or  for has.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111468</th>\n",
       "      <td>zymotic</td>\n",
       "      <td>Of  pertaining to or caused by fermentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111469</th>\n",
       "      <td>zyophyte</td>\n",
       "      <td>Any plant of a proposed class or grand divisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111470</th>\n",
       "      <td>zythem</td>\n",
       "      <td>See Zythum.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111471</th>\n",
       "      <td>zythepsary</td>\n",
       "      <td>A brewery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111472</th>\n",
       "      <td>zythum</td>\n",
       "      <td>A kind of ancient malt beverage; a liquor made...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111473 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                         definition\n",
       "0              'em  An obsolete or colloquial contraction of the o...\n",
       "1          'gainst                          A contraction of Against.\n",
       "2          'mongst                                       See Amongst.\n",
       "3           'neath                        An abbreviation of Beneath.\n",
       "4               's                  A contraction for is or  for has.\n",
       "...            ...                                                ...\n",
       "111468     zymotic       Of  pertaining to or caused by fermentation.\n",
       "111469    zyophyte  Any plant of a proposed class or grand divisio...\n",
       "111470      zythem                                        See Zythum.\n",
       "111471  zythepsary                                         A brewery.\n",
       "111472      zythum  A kind of ancient malt beverage; a liquor made...\n",
       "\n",
       "[111473 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions_filter = definitions.drop(['Count', 'POS'], axis=1)\n",
    "definitions_filter['word'] = definitions_filter['Word']\n",
    "definitions_filter['definition'] = definitions_filter['Definition']\n",
    "\n",
    "definitions_filter = definitions_filter.drop(['Word', 'Definition'], axis=1)\n",
    "definitions_filter = cleanDataframe(df=definitions_filter)\n",
    "\n",
    "regexQuote = \"^\\\"|\\\"$\"\n",
    "definitions_filter = definitions_filter.replace(\n",
    "    to_replace=regexQuote, value=\"\", regex=True)\n",
    "definitions_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLIFIED DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accessible</td>\n",
       "      <td>When something is accessible it means anyone c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>You accommodate when you change something that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accomplish</td>\n",
       "      <td>If you accomplish something, you succeed in do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>achieve</td>\n",
       "      <td>If you achieve something, you succeed in doing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acre</td>\n",
       "      <td>An acre is a very large area of land about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>value</td>\n",
       "      <td>The value of a place or thing is how much mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>verify</td>\n",
       "      <td>If youÂ verifyÂ something, you make sure that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>vigilant</td>\n",
       "      <td>Someone who isÂ vigilantÂ pays careful attention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>visible</td>\n",
       "      <td>When something is visible, you can see it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>wish</td>\n",
       "      <td>When you wish for something, you think about s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                         definition\n",
       "0     accessible  When something is accessible it means anyone c...\n",
       "1    accommodate  You accommodate when you change something that...\n",
       "2     accomplish  If you accomplish something, you succeed in do...\n",
       "3        achieve  If you achieve something, you succeed in doing...\n",
       "4           acre  An acre is a very large area of land about the...\n",
       "..           ...                                                ...\n",
       "162        value  The value of a place or thing is how much mone...\n",
       "163       verify  If youÂ verifyÂ something, you make sure that it...\n",
       "164     vigilant  Someone who isÂ vigilantÂ pays careful attention...\n",
       "165      visible         When something is visible, you can see it.\n",
       "166         wish  When you wish for something, you think about s...\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter simpDef1\n",
    "simpDef1_Filter = simpDef1.drop(['Exemplar'], axis=1)\n",
    "simpDef1_Filter['word'] = simpDef1_Filter['Word']\n",
    "simpDef1_Filter['definition'] = simpDef1_Filter['Child Friendly Definition']\n",
    "simpDef1_Filter = simpDef1_Filter.drop(\n",
    "    ['Word', 'Child Friendly Definition'], axis=1)\n",
    "simpDef1_Filter = cleanDataframe(simpDef1_Filter)\n",
    "simpDef1_Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s</td>\n",
       "      <td>a suffix used to form the possessive of most s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'tis</td>\n",
       "      <td>shortened form of \"it is.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'twas</td>\n",
       "      <td>shortened form of \"it was.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>the first letter of the English alphabet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a dime a dozen</td>\n",
       "      <td>plentiful and easy to get; common; cheap.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13907</th>\n",
       "      <td>zone</td>\n",
       "      <td>an area that is divided from other areas becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13908</th>\n",
       "      <td>zoo</td>\n",
       "      <td>a place where living animals, especially wild ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13909</th>\n",
       "      <td>zoology</td>\n",
       "      <td>the science and study of animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13910</th>\n",
       "      <td>zoom</td>\n",
       "      <td>to move quickly while making a low humming sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13911</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>a type of summer squash that is shaped like a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13912 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word                                         definition\n",
       "0                  's  a suffix used to form the possessive of most s...\n",
       "1                'tis                         shortened form of \"it is.\"\n",
       "2               'twas                        shortened form of \"it was.\"\n",
       "3                   a         the first letter of the English alphabet.Â \n",
       "4      a dime a dozen          plentiful and easy to get; common; cheap.\n",
       "...               ...                                                ...\n",
       "13907            zone  an area that is divided from other areas becau...\n",
       "13908             zoo  a place where living animals, especially wild ...\n",
       "13909         zoology                  the science and study of animals.\n",
       "13910            zoom  to move quickly while making a low humming sou...\n",
       "13911        zucchini  a type of summer squash that is shaped like a ...\n",
       "\n",
       "[13912 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpDef2_filter = simpDef2\n",
    "simpDef2_Filter = cleanDataframe(simpDef2_filter)\n",
    "simpDef2_Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Dataset ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, pd\u001b[38;5;241m.\u001b[39mDataFrame([[w, defs, simpDefs]], columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefinition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimplified_definition\u001b[39m\u001b[38;5;124m'\u001b[39m])], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m simpDef2_Filter\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 17\u001b[0m     defs \u001b[38;5;241m=\u001b[39m definitions_filter[\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdefinitions_filter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword\u001b[49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefinition\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m defs\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m         w \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Basically we are combining all the dataset together and putting it into a csv\n",
    "# Doing this because i don't want to constantly run this script (takes up RAM that I dont have)\n",
    "\n",
    "df = pd.DataFrame(columns=['word', 'definition', 'simplified_definition'])\n",
    "\n",
    "for index, row in simpDef1_Filter.iterrows():\n",
    "    defs = definitions_filter[row['word'] ==\n",
    "                              definitions_filter.word]['definition']\n",
    "    if defs.count() >= 1:\n",
    "        w = row['word']\n",
    "        simpDefs = row['definition']\n",
    "        defs = defs.values[0]\n",
    "        df = pd.concat([df, pd.DataFrame([[w, defs, simpDefs]], columns=[\n",
    "                       'word', 'definition', 'simplified_definition'])], ignore_index=True)\n",
    "\n",
    "for index, row in simpDef2_Filter.iterrows():\n",
    "    defs = definitions_filter[row['word'] ==\n",
    "                              definitions_filter.word]['definition']\n",
    "    if defs.count() >= 1:\n",
    "        w = row['word']\n",
    "        simpDefs = row['definition']\n",
    "        defs = defs.values[0]\n",
    "        df = pd.concat([df, pd.DataFrame([[w, defs, simpDefs]], columns=[\n",
    "                       'word', 'definition', 'simplified_definition'])], ignore_index=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mfullDataset.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"fullDataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "fullDs = pd.read_csv(\"fullDataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def convertTextToNumbers(words, tokenizer=None, padding=None, isTrainY=False):\n",
    "    # Create a tokenizer and fit it on the entire text corpus\n",
    "    if tokenizer == None:\n",
    "        #tokenizer = Tokenizer(split=' ')\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(words)\n",
    "\n",
    "    # Convert the text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(words)\n",
    "\n",
    "    if isTrainY:\n",
    "        tokenizer.word_index['<start>'] = len(\n",
    "            tokenizer.word_index) + 1\n",
    "        tokenizer.word_index['<end>'] = len(\n",
    "            tokenizer.word_index) + 1\n",
    "        sequences = [[tokenizer.word_index['<start>']] +\n",
    "                     seq + [tokenizer.word_index['<end>']] for seq in sequences]\n",
    "\n",
    "    # Get the word index mapping\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Making sure all inputs are of the same length\n",
    "    if padding == None:\n",
    "        sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, padding='post')\n",
    "    else:\n",
    "        sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, maxlen=padding, padding='post')\n",
    "    return (sequences, word_index, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m input_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(input_texts)\n\u001b[1;32m      9\u001b[0m max_len_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_seq)\n\u001b[0;32m---> 10\u001b[0m input_pad_seq \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m(input_seq, maxlen\u001b[38;5;241m=\u001b[39mmax_len_input, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m simp_eng_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m     13\u001b[0m simp_eng_tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(simplified_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "input_texts = ['dog is an animal', 'cat is a mammal', 'fish lives in water']\n",
    "simplified_texts = ['dog is animal', 'cat is mammal', 'fish lives in water']\n",
    "\n",
    "# Tokenize input and output data\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_seq = input_tokenizer.texts_to_sequences(input_texts)\n",
    "max_len_input = max(len(seq) for seq in input_seq)\n",
    "input_pad_seq = pad_sequences(input_seq, maxlen=max_len_input, padding='post')\n",
    "\n",
    "simp_eng_tokenizer = Tokenizer()\n",
    "simp_eng_tokenizer.fit_on_texts(simplified_texts)\n",
    "simp_eng_seq = simp_eng_tokenizer.texts_to_sequences(simplified_texts)\n",
    "max_len_output = max(len(seq) for seq in simp_eng_seq)\n",
    "simp_eng_pad_seq = pad_sequences(\n",
    "    simp_eng_seq, maxlen=max_len_output, padding='post')\n",
    "\n",
    "# Add start and end tokens to decoder input and target data\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(simplified_texts), max_len_output), dtype='int32')\n",
    "decoder_target_data = np.zeros((len(simplified_texts), max_len_output, len(\n",
    "    simp_eng_tokenizer.word_index) + 1), dtype='float32')\n",
    "\n",
    "\n",
    "for i, text in enumerate(simplified_texts):\n",
    "    tokens = ['<start>'] + text.split() + ['<end>']\n",
    "    for j, token in enumerate(tokens):\n",
    "        decoder_input_data[i, j] = simp_eng_tokenizer.word_index.get(token, 0)\n",
    "        if j > 0:\n",
    "            k = simp_eng_tokenizer.word_index.get(token, 0)\n",
    "            decoder_target_data[i, j - 1, k] = 1.0\n",
    "\n",
    "# Print example input and target data for the first sample\n",
    "print('Encoder input data:', input_pad_seq[0])\n",
    "print('Decoder input data:', decoder_input_data[0])\n",
    "print('Decoder target data:')\n",
    "for j in range(max_len_output):\n",
    "    print(simp_eng_tokenizer.index_word[np.argmax(\n",
    "        decoder_target_data[0, j])], end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set (do not need test) - we will provide a definition and evaluate it ourselves\n",
    "# we also may not need a validation set since i dont know if it will evaluate it properly.\n",
    "fullDs = fullDs.sample(frac=1)  # shuffle the dataset\n",
    "fullDs = fullDs.astype(str)\n",
    "\n",
    "# the two arrays below are tokenized and padded for the algorithm\n",
    "full_train_x, defWordIndex, defTokenizer = convertTextToNumbers(\n",
    "    fullDs['definition'].values[0:10000])\n",
    "\n",
    "full_train_y, simpDefWordIndex, simpDefTokenizer = convertTextToNumbers(\n",
    "    fullDs['simplified_definition'].values[0:10000], isTrainY=True)\n",
    "\n",
    "full_train_y_RAW = fullDs['simplified_definition'].values[0:10000]\n",
    "#decoder_target_data_y = createOneHotEncodingVector(full_train_y, len(simpDefWordIndex), simpDefWordIndex)\n",
    "#full_train_x = full_train_x.astype('int')\n",
    "#full_train_y = full_train_y.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10918"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fullDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6350, 2709, 3866,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8993,  733,   44,    3, 8994,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10918\n",
      "8000\n",
      "8000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_x = full_train_x[:int(len(full_train_x) * train_ratio)]\n",
    "train_y = full_train_y[:int(len(full_train_y) * train_ratio)]\n",
    "val_x = full_train_x[int(len(full_train_x) * train_ratio):]\n",
    "val_y = full_train_y[int(len(full_train_y) * train_ratio):]\n",
    "\n",
    "# Convert the Pandas DataFrame to TensorFlow Dataset\n",
    "#train_ds = tf.data.Dataset.from_tensor_slices((train_df.values[:, :-1], train_df.values[:, -1]))\n",
    "#val_ds = tf.data.Dataset.from_tensor_slices((val_df.values[:, :-1], val_df.values[:, -1]))\n",
    "print(len(fullDs['definition']))  # 10918\n",
    "print(len(train_x))  # 8734\n",
    "print(len(train_y))  # 8734\n",
    "print(len(val_x))  # 2184\n",
    "print(len(val_y))  # 2184\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6350 2709 3866    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[8691    6  533  210    3 8692    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "adherence attachment devotion\n",
      "to stick closely Â \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "      <th>simplified_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>cling</td>\n",
       "      <td>Adherence; attachment; devotion.</td>\n",
       "      <td>to stick closely.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                        definition simplified_definition\n",
       "1987  cling  Adherence; attachment; devotion.    to stick closely.Â "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_train_x[0])  # def (should be head(1))\n",
    "print(full_train_y[0])  # simp def\n",
    "\n",
    "print(defTokenizer.sequences_to_texts([full_train_x[0]])[0])\n",
    "print(simpDefTokenizer.sequences_to_texts([full_train_y[0]])[0])\n",
    "print(\"\\n\\n\\n\")\n",
    "fullDs.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17600\n",
      "8692\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 100)    1760000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    869200      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 170),        184280      ['embedding[0][0]']              \n",
      "                                 (None, 170),                                                     \n",
      "                                 (None, 170)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 170),  184280      ['embedding_1[0][0]',            \n",
      "                                 (None, 170),                     'lstm[0][1]',                   \n",
      "                                 (None, 170)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 8692)  1486332     ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,484,092\n",
      "Trainable params: 4,484,092\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(encoding_token, decoding_token, embedding_dim, latent_dim):\n",
    "\n",
    "    # Define input sequence\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define encoder embedding layer\n",
    "    enc_emb = Embedding(input_dim=encoding_token,\n",
    "                        output_dim=embedding_dim)(inputs)\n",
    "\n",
    "    # Define encoder LSTM\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    # Discard encoder outputs and only keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Define decoder input sequence\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define decoder embedding layer\n",
    "    dec_emb_layer = Embedding(input_dim=decoding_token,\n",
    "                              output_dim=embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Define decoder LSTM\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Define decoder output layer\n",
    "    #flat = Flatten()(decoder_outputs)\n",
    "\n",
    "    #decoder_dense = Dense(decoding_token, activation='softmax')\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(decoding_token, activation='linear'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "encoding_token = len(defWordIndex)\n",
    "decoding_token = len(simpDefWordIndex)\n",
    "print(encoding_token)\n",
    "print(decoding_token)\n",
    "# usually 50 - 500 higher = more complex relation but higher chance of overfitting.\n",
    "embedding_dim = 100\n",
    "# usually 128 - 256 higher = more complex relation but higher chance of overfitting.\n",
    "latent_dim = 170\n",
    "model = createModel(encoding_token=encoding_token, decoding_token=decoding_token,\n",
    "                    embedding_dim=embedding_dim, latent_dim=latent_dim)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17601\n",
      "8693\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 128)    2252928     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 50)     434650      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 128),        131584      ['embedding_2[0][0]']            \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 128),  91648       ['embedding_3[0][0]',            \n",
      "                                 (None, 128),                     'lstm_2[0][1]',                 \n",
      "                                 (None, 128)]                     'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 8693)  1121397     ['lstm_3[0][0]']                 \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,032,207\n",
      "Trainable params: 4,032,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# use this one\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(encoding_token, decoding_token, embedding_dim, latent_dim):\n",
    "\n",
    "    # Define input sequence\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define encoder embedding layer\n",
    "    enc_emb = Embedding(encoding_token, latent_dim, mask_zero=True)(inputs)\n",
    "\n",
    "    # Define encoder LSTM\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    # Discard encoder outputs and only keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Define decoder input sequence\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define decoder embedding layer\n",
    "    dec_emb_layer = Embedding(decoding_token,\n",
    "                              output_dim=embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Define decoder LSTM\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Define decoder output layer\n",
    "    # flat = Flatten()(decoder_outputs)\n",
    "\n",
    "    # decoder_dense = Dense(decoding_token, activation='softmax')\n",
    "\n",
    "    decoder_dense = TimeDistributed(\n",
    "        Dense(decoding_token, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "encoding_token = len(defWordIndex) + 1\n",
    "decoding_token = len(simpDefWordIndex) + 1\n",
    "print(encoding_token)\n",
    "print(decoding_token)\n",
    "# usually 50 - 500 higher = more complex relation but higher chance of overfitting.\n",
    "embedding_dim = 50\n",
    "# usually 128 - 256 higher = more complex relation but higher chance of overfitting.\n",
    "latent_dim = 128\n",
    "model = createModel(encoding_token=encoding_token, decoding_token=decoding_token,\n",
    "                    embedding_dim=embedding_dim, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "period\n",
      "between\n",
      "late\n",
      "afternoon\n",
      "and\n",
      "nightfall.Â \n",
      "to\n",
      "climb\n",
      "with\n",
      "difficulty\n",
      "or\n",
      "in\n",
      "an\n",
      "awkward\n",
      "way,\n",
      "using\n",
      "hands\n",
      "and\n",
      "feet.\n",
      "completely;\n",
      "entirely.\n",
      "loose,\n",
      "light,\n",
      "fluffy\n",
      "matter\n",
      "such\n",
      "as\n",
      "fibers\n",
      "or\n",
      "hairs.\n",
      "not\n",
      "valuable\n",
      "or\n",
      "important;\n",
      "insignificant.\n",
      "not\n",
      "containing\n",
      "anything;\n",
      "empty.Â \n",
      "to\n",
      "hold\n",
      "or\n",
      "squeeze\n",
      "with\n",
      "the\n",
      "arms\n",
      "in\n",
      "a\n",
      "loving\n",
      "way;\n",
      "embrace.Â \n",
      "soldiers\n",
      "on\n",
      "foot,\n",
      "or\n",
      "the\n",
      "branch\n",
      "of\n",
      "the\n",
      "military\n",
      "to\n",
      "which\n",
      "they\n",
      "belong.\n",
      "being\n",
      "the\n",
      "place\n",
      "of\n",
      "birth\n",
      "or\n",
      "origin.Â \n",
      "the\n",
      "position\n",
      "between\n",
      "second\n",
      "and\n",
      "third\n",
      "base\n",
      "in\n",
      "baseball\n",
      "or\n",
      "softball,\n",
      "or\n",
      "the\n",
      "player\n",
      "in\n",
      "this\n",
      "position.\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(full_train_y_RAW[0:10]):\n",
    "    tokens = sentence.split(' ')\n",
    "    for j, token in enumerate(tokens):\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 14:27:53.318053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:27:53.326475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:27:53.326716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(input_sequences, target_sequences, target_RAW, batch_size, simpWordIndex, lengthOfSentence):\n",
    "    while True:\n",
    "        for i in range(0, len(target_RAW), batch_size):\n",
    "\n",
    "            decoder_target_data = np.zeros(\n",
    "                (batch_size, lengthOfSentence, len(simpWordIndex)+1), dtype='float32')\n",
    "            for index, sentence in enumerate(target_RAW[i:i+batch_size]):\n",
    "                #print(sentence)\n",
    "                tokens = sentence.split(' ')\n",
    "                for j, token in enumerate(tokens):\n",
    "                    temp = simpWordIndex.get(token, 0)\n",
    "                    if temp > 0:\n",
    "\n",
    "                        decoder_target_data[index, j, temp] = 1.0\n",
    "                        \n",
    "            yield [input_sequences[i:i+batch_size], target_sequences[i:i+batch_size]], decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 10:18:58.128344: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 173860000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 10:19:04.932970: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 173860000 exceeds 10% of free system memory.\n",
      "2023-03-16 10:19:05.189789: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 173860000 exceeds 10% of free system memory.\n",
      "2023-03-16 10:19:05.314768: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_40/output/_23'\n",
      "2023-03-16 10:19:11.545986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-16 10:19:22.440965: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f89f40052f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-16 10:19:22.480204: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "2023-03-16 10:19:23.577314: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-16 10:19:29.449288: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/200 [..............................] - ETA: 1:25 - loss: 0.8934   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 10:19:31.440578: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 173860000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/200 [..............................] - ETA: 1:22 - loss: 0.8803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 10:19:31.821397: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 173860000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 95s 317ms/step - loss: 0.6178\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 45s 226ms/step - loss: 0.5455\n",
      "Epoch 3/100\n",
      " 14/200 [=>............................] - ETA: 44s - loss: 0.5293"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m steps_per_epoch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(full_train_x) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size\n\u001b[1;32m      4\u001b[0m generator \u001b[39m=\u001b[39m data_generator(full_train_x, full_train_y, full_train_y_RAW,\n\u001b[1;32m      5\u001b[0m                            batch_size, simpDefWordIndex, \u001b[39mlen\u001b[39m(full_train_y[\u001b[39m0\u001b[39m]))\n\u001b[0;32m----> 6\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "steps_per_epoch = len(full_train_x) // batch_size\n",
    "\n",
    "generator = data_generator(full_train_x, full_train_y, full_train_y_RAW,\n",
    "                           batch_size, simpDefWordIndex, len(full_train_y[0]))\n",
    "history = model.fit(generator, epochs=100, batch_size=batch_size, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOneHotEncodingVector(simpDef, maxLengthOutput, wordIndex):\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(simpDef), maxLengthOutput, len(wordIndex) + 1), dtype='float32')\n",
    "    for i, word in enumerate(simpDef):\n",
    "        for j, token in enumerate(word):\n",
    "            index = wordIndex.get(token, 0)\n",
    "            if index > 0:\n",
    "                decoder_target_data[i, j-1, index] == 1.0\n",
    "    return decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simpDefWordIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at one time in the past; formerly.\\xa0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y_RAW[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = full_train_y_RAW[0].split(' ')[0]\n",
    "print(t)\n",
    "simpDefWordIndex.get(t, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "new_model = tf.keras.models.load_model(\n",
    "    './model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  629  2244 16425  2983     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "['west african ceremonial trumpet']\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[[[1.07231727e-07 1.87638253e-01 5.02054952e-02 ... 9.97568748e-08\n",
      "   1.07032086e-07 1.20764909e-07]\n",
      "  [5.73666625e-07 8.70829001e-02 5.92710041e-02 ... 5.62546234e-07\n",
      "   5.34377591e-07 6.51039613e-07]\n",
      "  [3.82859270e-07 6.07789606e-02 8.60708058e-02 ... 3.76094505e-07\n",
      "   3.50886751e-07 4.42238672e-07]\n",
      "  ...\n",
      "  [3.27401011e-07 5.45374043e-02 9.38221142e-02 ... 3.20531910e-07\n",
      "   2.98299938e-07 3.83179270e-07]\n",
      "  [3.27401011e-07 5.45374043e-02 9.38221142e-02 ... 3.20531910e-07\n",
      "   2.98299938e-07 3.83179270e-07]\n",
      "  [3.27401011e-07 5.45374043e-02 9.38221142e-02 ... 3.20531910e-07\n",
      "   2.98299938e-07 3.83179270e-07]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data, tokens, tokenizer = convertTextToNumbers(\n",
    "    [\"West African ceremonial trumpet\"], defTokenizer, padding=len(full_train_x[0]))\n",
    "print(data)\n",
    "print(defTokenizer.sequences_to_texts(data))\n",
    "\n",
    "inputdata = np.array(data).reshape(1, -1)\n",
    "\n",
    "outputdata = np.zeros(shape=(1, len(full_train_y[0])))\n",
    "outputdata[0, 0] = simpDefTokenizer.word_index['<start>']\n",
    "#outputdata = np.reshape(outputdata, (1,1))\n",
    "\n",
    "\n",
    "newSentence = model.predict([inputdata, outputdata])\n",
    "print(newSentence)\n",
    "#print(simpDefTokenizer.sequences_to_texts(newSentence))\n",
    "\n",
    "# Assuming you have already preprocessed and tokenized the sentence\n",
    "#test_sentence = \"the cat jumped over the moon\"\n",
    "#test_sentence_tokens = tokenizer.texts_to_sequences([test_sentence])[0]\n",
    "#test_sentence_tokens = np.array(test_sentence_tokens).reshape(1, -1)\n",
    "\n",
    "# Initialize the decoder input with the start token\n",
    "#decoder_input = np.zeros(shape=(1, max_summary_len))\n",
    "#decoder_input[0, 0] = summary_tokenizer.word_index['start']\n",
    "\n",
    "# Reshape the decoder input to have a length of 1\n",
    "#decoder_input = np.reshape(decoder_input, (1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convertPredictionToSentence(ds, tokenizer):\n",
    "    finalSentence = np.zeros((len(ds), len(ds[0])), dtype='int32')\n",
    "    for i, sentence in enumerate(ds):\n",
    "        for j, word in enumerate(sentence):\n",
    "            maxIndex = np.argmax(word, axis=0)\n",
    "            print(maxIndex)\n",
    "            finalSentence[i,j] = maxIndex\n",
    "\n",
    "    return tokenizer.sequences_to_texts(finalSentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a a or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or or']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,6,3,8,23,5,9,4,100],[1,6,3,8,3,5,9,4,50],[1,50,3,8,23,5,9,4,25]]])\n",
    "convertPredictionToSentence(newSentence, simpDefTokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting/Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m val_accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a675ecb475794e98cc508edff942b4df7efb840dcd0161848ec7416e01ca0315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
