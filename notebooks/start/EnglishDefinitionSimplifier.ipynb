{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 14:27:33.874681: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tfgroup/anaconda3/envs/tf/lib/:/home/tfgroup/anaconda3/envs/tf/lib/\n",
      "2023-03-13 14:27:33.874775: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tfgroup/anaconda3/envs/tf/lib/:/home/tfgroup/anaconda3/envs/tf/lib/\n",
      "2023-03-13 14:27:33.874785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 14:25:14.974461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.120001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.120421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.122485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.122871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:15.123198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.303576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.303971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.304312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:25:18.304563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2254 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 3GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# old model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Rescaling(1./255),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "definitions = pd.read_csv('OPTED-Dictionary.csv')\n",
    "simpDef1 = pd.read_excel('ChildFriendlyDefinitions.xlsx', sheet_name='Sheet1')\n",
    "simpDef2 = pd.read_json('data.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING (Everything below this only needs to be run once. A CSV will be created with the full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cleanDataframe(df):\n",
    "    df_copy = df\n",
    "\n",
    "    # may need to take the '-' out\n",
    "    regex = \"\\[(.*?)\\]|[0-9!@#$%^&*?\\/=+\\-]|\\((.*?)\\)|\\{(.*?)\\}|\\<(.*?)\\>\"\n",
    "\n",
    "    df_copy = df_copy.replace(\n",
    "        to_replace=regex, value=\"\", regex=True).dropna()  # remove illegal chars\n",
    "\n",
    "    df_copy.word = df_copy.word.str.lower()  # lower case everything\n",
    "\n",
    "    df_copy = df_copy.sort_values('word', ascending=True)\n",
    "    df_copy = df_copy.drop_duplicates(subset='word', keep='first')\n",
    "\n",
    "    return df_copy.reset_index().drop(['index'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ACTUAL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'em</td>\n",
       "      <td>An obsolete or colloquial contraction of the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'gainst</td>\n",
       "      <td>A contraction of Against.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'mongst</td>\n",
       "      <td>See Amongst.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'neath</td>\n",
       "      <td>An abbreviation of Beneath.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'s</td>\n",
       "      <td>A contraction for is or  for has.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111468</th>\n",
       "      <td>zymotic</td>\n",
       "      <td>Of  pertaining to or caused by fermentation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111469</th>\n",
       "      <td>zyophyte</td>\n",
       "      <td>Any plant of a proposed class or grand divisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111470</th>\n",
       "      <td>zythem</td>\n",
       "      <td>See Zythum.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111471</th>\n",
       "      <td>zythepsary</td>\n",
       "      <td>A brewery.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111472</th>\n",
       "      <td>zythum</td>\n",
       "      <td>A kind of ancient malt beverage; a liquor made...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111473 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word                                         definition\n",
       "0              'em  An obsolete or colloquial contraction of the o...\n",
       "1          'gainst                          A contraction of Against.\n",
       "2          'mongst                                       See Amongst.\n",
       "3           'neath                        An abbreviation of Beneath.\n",
       "4               's                  A contraction for is or  for has.\n",
       "...            ...                                                ...\n",
       "111468     zymotic       Of  pertaining to or caused by fermentation.\n",
       "111469    zyophyte  Any plant of a proposed class or grand divisio...\n",
       "111470      zythem                                        See Zythum.\n",
       "111471  zythepsary                                         A brewery.\n",
       "111472      zythum  A kind of ancient malt beverage; a liquor made...\n",
       "\n",
       "[111473 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions_filter = definitions.drop(['Count', 'POS'], axis=1)\n",
    "definitions_filter['word'] = definitions_filter['Word']\n",
    "definitions_filter['definition'] = definitions_filter['Definition']\n",
    "\n",
    "definitions_filter = definitions_filter.drop(['Word', 'Definition'], axis=1)\n",
    "definitions_filter = cleanDataframe(df=definitions_filter)\n",
    "\n",
    "regexQuote = \"^\\\"|\\\"$\"\n",
    "definitions_filter = definitions_filter.replace(\n",
    "    to_replace=regexQuote, value=\"\", regex=True)\n",
    "definitions_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLIFIED DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accessible</td>\n",
       "      <td>When something is accessible it means anyone c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>You accommodate when you change something that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accomplish</td>\n",
       "      <td>If you accomplish something, you succeed in do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>achieve</td>\n",
       "      <td>If you achieve something, you succeed in doing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acre</td>\n",
       "      <td>An acre is a very large area of land about the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>value</td>\n",
       "      <td>The value of a place or thing is how much mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>verify</td>\n",
       "      <td>If youÂ verifyÂ something, you make sure that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>vigilant</td>\n",
       "      <td>Someone who isÂ vigilantÂ pays careful attention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>visible</td>\n",
       "      <td>When something is visible, you can see it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>wish</td>\n",
       "      <td>When you wish for something, you think about s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                         definition\n",
       "0     accessible  When something is accessible it means anyone c...\n",
       "1    accommodate  You accommodate when you change something that...\n",
       "2     accomplish  If you accomplish something, you succeed in do...\n",
       "3        achieve  If you achieve something, you succeed in doing...\n",
       "4           acre  An acre is a very large area of land about the...\n",
       "..           ...                                                ...\n",
       "162        value  The value of a place or thing is how much mone...\n",
       "163       verify  If youÂ verifyÂ something, you make sure that it...\n",
       "164     vigilant  Someone who isÂ vigilantÂ pays careful attention...\n",
       "165      visible         When something is visible, you can see it.\n",
       "166         wish  When you wish for something, you think about s...\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter simpDef1\n",
    "simpDef1_Filter = simpDef1.drop(['Exemplar'], axis=1)\n",
    "simpDef1_Filter['word'] = simpDef1_Filter['Word']\n",
    "simpDef1_Filter['definition'] = simpDef1_Filter['Child Friendly Definition']\n",
    "simpDef1_Filter = simpDef1_Filter.drop(\n",
    "    ['Word', 'Child Friendly Definition'], axis=1)\n",
    "simpDef1_Filter = cleanDataframe(simpDef1_Filter)\n",
    "simpDef1_Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'s</td>\n",
       "      <td>a suffix used to form the possessive of most s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'tis</td>\n",
       "      <td>shortened form of \"it is.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'twas</td>\n",
       "      <td>shortened form of \"it was.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>the first letter of the English alphabet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a dime a dozen</td>\n",
       "      <td>plentiful and easy to get; common; cheap.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13907</th>\n",
       "      <td>zone</td>\n",
       "      <td>an area that is divided from other areas becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13908</th>\n",
       "      <td>zoo</td>\n",
       "      <td>a place where living animals, especially wild ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13909</th>\n",
       "      <td>zoology</td>\n",
       "      <td>the science and study of animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13910</th>\n",
       "      <td>zoom</td>\n",
       "      <td>to move quickly while making a low humming sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13911</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>a type of summer squash that is shaped like a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13912 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word                                         definition\n",
       "0                  's  a suffix used to form the possessive of most s...\n",
       "1                'tis                         shortened form of \"it is.\"\n",
       "2               'twas                        shortened form of \"it was.\"\n",
       "3                   a         the first letter of the English alphabet.Â \n",
       "4      a dime a dozen          plentiful and easy to get; common; cheap.\n",
       "...               ...                                                ...\n",
       "13907            zone  an area that is divided from other areas becau...\n",
       "13908             zoo  a place where living animals, especially wild ...\n",
       "13909         zoology                  the science and study of animals.\n",
       "13910            zoom  to move quickly while making a low humming sou...\n",
       "13911        zucchini  a type of summer squash that is shaped like a ...\n",
       "\n",
       "[13912 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpDef2_filter = simpDef2\n",
    "simpDef2_Filter = cleanDataframe(simpDef2_filter)\n",
    "simpDef2_Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Dataset ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, pd\u001b[38;5;241m.\u001b[39mDataFrame([[w, defs, simpDefs]], columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefinition\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimplified_definition\u001b[39m\u001b[38;5;124m'\u001b[39m])], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m simpDef2_Filter\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 17\u001b[0m     defs \u001b[38;5;241m=\u001b[39m definitions_filter[\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mdefinitions_filter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword\u001b[49m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefinition\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m defs\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     20\u001b[0m         w \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[38;5;241m=\u001b[39m libops\u001b[38;5;241m.\u001b[39mvec_compare(x\u001b[38;5;241m.\u001b[39mravel(), y\u001b[38;5;241m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Basically we are combining all the dataset together and putting it into a csv\n",
    "# Doing this because i don't want to constantly run this script (takes up RAM that I dont have)\n",
    "\n",
    "df = pd.DataFrame(columns=['word', 'definition', 'simplified_definition'])\n",
    "\n",
    "for index, row in simpDef1_Filter.iterrows():\n",
    "    defs = definitions_filter[row['word'] ==\n",
    "                              definitions_filter.word]['definition']\n",
    "    if defs.count() >= 1:\n",
    "        w = row['word']\n",
    "        simpDefs = row['definition']\n",
    "        defs = defs.values[0]\n",
    "        df = pd.concat([df, pd.DataFrame([[w, defs, simpDefs]], columns=[\n",
    "                       'word', 'definition', 'simplified_definition'])], ignore_index=True)\n",
    "\n",
    "for index, row in simpDef2_Filter.iterrows():\n",
    "    defs = definitions_filter[row['word'] ==\n",
    "                              definitions_filter.word]['definition']\n",
    "    if defs.count() >= 1:\n",
    "        w = row['word']\n",
    "        simpDefs = row['definition']\n",
    "        defs = defs.values[0]\n",
    "        df = pd.concat([df, pd.DataFrame([[w, defs, simpDefs]], columns=[\n",
    "                       'word', 'definition', 'simplified_definition'])], ignore_index=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mfullDataset.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"fullDataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "fullDs = pd.read_csv(\"fullDataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def convertTextToNumbers(words, tokenizer=None, padding=None, isTrainY=False):\n",
    "    # Create a tokenizer and fit it on the entire text corpus\n",
    "    if tokenizer == None:\n",
    "        #tokenizer = Tokenizer(split=' ')\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(words)\n",
    "\n",
    "    # Convert the text to sequences of integers\n",
    "    sequences = tokenizer.texts_to_sequences(words)\n",
    "\n",
    "    if isTrainY:\n",
    "        tokenizer.word_index['<start>'] = len(\n",
    "            tokenizer.word_index) + 1\n",
    "        tokenizer.word_index['<end>'] = len(\n",
    "            tokenizer.word_index) + 1\n",
    "        sequences = [[tokenizer.word_index['<start>']] +\n",
    "                     seq + [tokenizer.word_index['<end>']] for seq in sequences]\n",
    "\n",
    "    # Get the word index mapping\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Making sure all inputs are of the same length\n",
    "    if padding == None:\n",
    "        sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, padding='post')\n",
    "    else:\n",
    "        sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            sequences, maxlen=padding, padding='post')\n",
    "    return (sequences, word_index, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pad_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m input_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(input_texts)\n\u001b[1;32m      9\u001b[0m max_len_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m input_seq)\n\u001b[0;32m---> 10\u001b[0m input_pad_seq \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m(input_seq, maxlen\u001b[38;5;241m=\u001b[39mmax_len_input, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m simp_eng_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[1;32m     13\u001b[0m simp_eng_tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(simplified_texts)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pad_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "input_texts = ['dog is an animal', 'cat is a mammal', 'fish lives in water']\n",
    "simplified_texts = ['dog is animal', 'cat is mammal', 'fish lives in water']\n",
    "\n",
    "# Tokenize input and output data\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_seq = input_tokenizer.texts_to_sequences(input_texts)\n",
    "max_len_input = max(len(seq) for seq in input_seq)\n",
    "input_pad_seq = pad_sequences(input_seq, maxlen=max_len_input, padding='post')\n",
    "\n",
    "simp_eng_tokenizer = Tokenizer()\n",
    "simp_eng_tokenizer.fit_on_texts(simplified_texts)\n",
    "simp_eng_seq = simp_eng_tokenizer.texts_to_sequences(simplified_texts)\n",
    "max_len_output = max(len(seq) for seq in simp_eng_seq)\n",
    "simp_eng_pad_seq = pad_sequences(\n",
    "    simp_eng_seq, maxlen=max_len_output, padding='post')\n",
    "\n",
    "# Add start and end tokens to decoder input and target data\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(simplified_texts), max_len_output), dtype='int32')\n",
    "decoder_target_data = np.zeros((len(simplified_texts), max_len_output, len(\n",
    "    simp_eng_tokenizer.word_index) + 1), dtype='float32')\n",
    "\n",
    "\n",
    "for i, text in enumerate(simplified_texts):\n",
    "    tokens = ['<start>'] + text.split() + ['<end>']\n",
    "    for j, token in enumerate(tokens):\n",
    "        decoder_input_data[i, j] = simp_eng_tokenizer.word_index.get(token, 0)\n",
    "        if j > 0:\n",
    "            k = simp_eng_tokenizer.word_index.get(token, 0)\n",
    "            decoder_target_data[i, j - 1, k] = 1.0\n",
    "\n",
    "# Print example input and target data for the first sample\n",
    "print('Encoder input data:', input_pad_seq[0])\n",
    "print('Decoder input data:', decoder_input_data[0])\n",
    "print('Decoder target data:')\n",
    "for j in range(max_len_output):\n",
    "    print(simp_eng_tokenizer.index_word[np.argmax(\n",
    "        decoder_target_data[0, j])], end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set (do not need test) - we will provide a definition and evaluate it ourselves\n",
    "# we also may not need a validation set since i dont know if it will evaluate it properly.\n",
    "fullDs = fullDs.sample(frac=1)  # shuffle the dataset\n",
    "fullDs = fullDs.astype(str)\n",
    "\n",
    "# the two arrays below are tokenized and padded for the algorithm\n",
    "full_train_x, defWordIndex, defTokenizer = convertTextToNumbers(\n",
    "    fullDs['definition'].values[0:10000])\n",
    "\n",
    "full_train_y, simpDefWordIndex, simpDefTokenizer = convertTextToNumbers(\n",
    "    fullDs['simplified_definition'].values[0:10000], isTrainY=True)\n",
    "\n",
    "full_train_y_RAW = fullDs['simplified_definition'].values[0:10000]\n",
    "#decoder_target_data_y = createOneHotEncodingVector(full_train_y, len(simpDefWordIndex), simpDefWordIndex)\n",
    "#full_train_x = full_train_x.astype('int')\n",
    "#full_train_y = full_train_y.astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10918"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fullDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,   34,    2,   27,  180, 1542, 1543,  504,   44,  801, 1544,\n",
       "        505,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8993,  733,   44,    3, 8994,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10918\n",
      "8000\n",
      "8000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_x = full_train_x[:int(len(full_train_x) * train_ratio)]\n",
    "train_y = full_train_y[:int(len(full_train_y) * train_ratio)]\n",
    "val_x = full_train_x[int(len(full_train_x) * train_ratio):]\n",
    "val_y = full_train_y[int(len(full_train_y) * train_ratio):]\n",
    "\n",
    "# Convert the Pandas DataFrame to TensorFlow Dataset\n",
    "#train_ds = tf.data.Dataset.from_tensor_slices((train_df.values[:, :-1], train_df.values[:, -1]))\n",
    "#val_ds = tf.data.Dataset.from_tensor_slices((val_df.values[:, :-1], val_df.values[:, -1]))\n",
    "print(len(fullDs['definition']))  # 10918\n",
    "print(len(train_x))  # 8734\n",
    "print(len(train_y))  # 8734\n",
    "print(len(val_x))  # 2184\n",
    "print(len(val_y))  # 2184\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30 155  30 156 157   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n",
      "[489   8   1  56 123  57 124   3 490   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "more properly more correctly speaking\n",
      "in a more willing way sooner Â \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "      <th>simplified_definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8032</th>\n",
       "      <td>rather</td>\n",
       "      <td>More properly; more correctly speaking.</td>\n",
       "      <td>in a more willing way; sooner.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word                               definition  \\\n",
       "8032  rather  More properly; more correctly speaking.   \n",
       "\n",
       "                simplified_definition  \n",
       "8032  in a more willing way; sooner.Â   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_train_x[0])  # def (should be head(1))\n",
    "print(full_train_y[0])  # simp def\n",
    "\n",
    "print(defTokenizer.sequences_to_texts([full_train_x[0]])[0])\n",
    "print(simpDefTokenizer.sequences_to_texts([full_train_y[0]])[0])\n",
    "print(\"\\n\\n\\n\")\n",
    "fullDs.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734\n",
      "531\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(encoding_token, decoding_token, embedding_dim, latent_dim):\n",
    "\n",
    "    # Define input sequence\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define encoder embedding layer\n",
    "    enc_emb = Embedding(input_dim=encoding_token,\n",
    "                        output_dim=embedding_dim)(inputs)\n",
    "\n",
    "    # Define encoder LSTM\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    # Discard encoder outputs and only keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Define decoder input sequence\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define decoder embedding layer\n",
    "    dec_emb_layer = Embedding(input_dim=decoding_token,\n",
    "                              output_dim=embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Define decoder LSTM\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Define decoder output layer\n",
    "    #flat = Flatten()(decoder_outputs)\n",
    "\n",
    "    #decoder_dense = Dense(decoding_token, activation='softmax')\n",
    "\n",
    "    decoder_dense = TimeDistributed(Dense(decoding_token, activation='linear'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "encoding_token = len(defWordIndex)\n",
    "decoding_token = len(simpDefWordIndex)\n",
    "print(encoding_token)\n",
    "print(decoding_token)\n",
    "# usually 50 - 500 higher = more complex relation but higher chance of overfitting.\n",
    "embedding_dim = 100\n",
    "# usually 128 - 256 higher = more complex relation but higher chance of overfitting.\n",
    "latent_dim = 170\n",
    "model = createModel(encoding_token=encoding_token, decoding_token=decoding_token,\n",
    "                    embedding_dim=embedding_dim, latent_dim=latent_dim)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17606\n",
      "8686\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_16 (Embedding)       (None, None, 128)    2253568     ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_17 (Embedding)       (None, None, 50)     434300      ['input_18[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 [(None, 128),        131584      ['embedding_16[0][0]']           \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 [(None, None, 128),  91648       ['embedding_17[0][0]',           \n",
      "                                 (None, 128),                     'lstm_16[0][1]',                \n",
      "                                 (None, 128)]                     'lstm_16[0][2]']                \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDistri  (None, None, 8686)  1120494     ['lstm_17[0][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,031,594\n",
      "Trainable params: 4,031,594\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# use this one\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Lambda\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def createModel(encoding_token, decoding_token, embedding_dim, latent_dim):\n",
    "\n",
    "    # Define input sequence\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define encoder embedding layer\n",
    "    enc_emb = Embedding(encoding_token, latent_dim, mask_zero=True)(inputs)\n",
    "\n",
    "    # Define encoder LSTM\n",
    "    encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "    _, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "    # Discard encoder outputs and only keep states\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Define decoder input sequence\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # Define decoder embedding layer\n",
    "    dec_emb_layer = Embedding(decoding_token,\n",
    "                              output_dim=embedding_dim)\n",
    "    dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "    # Define decoder LSTM\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "\n",
    "    # Define decoder output layer\n",
    "    # flat = Flatten()(decoder_outputs)\n",
    "\n",
    "    # decoder_dense = Dense(decoding_token, activation='softmax')\n",
    "\n",
    "    decoder_dense = TimeDistributed(\n",
    "        Dense(decoding_token, activation='softmax'))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    # Define the model\n",
    "    model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "encoding_token = len(defWordIndex) + 1\n",
    "decoding_token = len(simpDefWordIndex) + 1\n",
    "print(encoding_token)\n",
    "print(decoding_token)\n",
    "# usually 50 - 500 higher = more complex relation but higher chance of overfitting.\n",
    "embedding_dim = 50\n",
    "# usually 128 - 256 higher = more complex relation but higher chance of overfitting.\n",
    "latent_dim = 128\n",
    "model = createModel(encoding_token=encoding_token, decoding_token=decoding_token,\n",
    "                    embedding_dim=embedding_dim, latent_dim=latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "period\n",
      "between\n",
      "late\n",
      "afternoon\n",
      "and\n",
      "nightfall.Â \n",
      "to\n",
      "climb\n",
      "with\n",
      "difficulty\n",
      "or\n",
      "in\n",
      "an\n",
      "awkward\n",
      "way,\n",
      "using\n",
      "hands\n",
      "and\n",
      "feet.\n",
      "completely;\n",
      "entirely.\n",
      "loose,\n",
      "light,\n",
      "fluffy\n",
      "matter\n",
      "such\n",
      "as\n",
      "fibers\n",
      "or\n",
      "hairs.\n",
      "not\n",
      "valuable\n",
      "or\n",
      "important;\n",
      "insignificant.\n",
      "not\n",
      "containing\n",
      "anything;\n",
      "empty.Â \n",
      "to\n",
      "hold\n",
      "or\n",
      "squeeze\n",
      "with\n",
      "the\n",
      "arms\n",
      "in\n",
      "a\n",
      "loving\n",
      "way;\n",
      "embrace.Â \n",
      "soldiers\n",
      "on\n",
      "foot,\n",
      "or\n",
      "the\n",
      "branch\n",
      "of\n",
      "the\n",
      "military\n",
      "to\n",
      "which\n",
      "they\n",
      "belong.\n",
      "being\n",
      "the\n",
      "place\n",
      "of\n",
      "birth\n",
      "or\n",
      "origin.Â \n",
      "the\n",
      "position\n",
      "between\n",
      "second\n",
      "and\n",
      "third\n",
      "base\n",
      "in\n",
      "baseball\n",
      "or\n",
      "softball,\n",
      "or\n",
      "the\n",
      "player\n",
      "in\n",
      "this\n",
      "position.\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in enumerate(full_train_y_RAW[0:10]):\n",
    "    tokens = sentence.split(' ')\n",
    "    for j, token in enumerate(tokens):\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 14:27:53.318053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:27:53.326475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-13 14:27:53.326716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(input_sequences, target_sequences, target_RAW, batch_size, simpWordIndex, lengthOfSentence):\n",
    "    while True:\n",
    "        for i in range(0, len(target_RAW), batch_size):\n",
    "\n",
    "            decoder_target_data = np.zeros(\n",
    "                (batch_size, lengthOfSentence, len(simpWordIndex)+1), dtype='float32')\n",
    "            for index, sentence in enumerate(target_RAW[i:i+batch_size]):\n",
    "                #print(sentence)\n",
    "                tokens = sentence.split(' ')\n",
    "                for j, token in enumerate(tokens):\n",
    "                    temp = simpWordIndex.get(token, 0)\n",
    "                    if temp > 0:\n",
    "\n",
    "                        decoder_target_data[index, j, temp] = 1.0\n",
    "                        \n",
    "            yield [input_sequences[i:i+batch_size], target_sequences[i:i+batch_size]], decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 75s 330ms/step - loss: 0.6140\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 37s 187ms/step - loss: 0.5426\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.5182\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.4985\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.4832\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.4706\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.4590\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.4482\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.4380\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.4284\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.4194\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.4108\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.4025\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 0.3944\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.3865\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 0.3793\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.3723\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.3658\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.3592\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 0.3524\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 0.3458\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.3395\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.3335\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.3276\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 0.3216\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.3156\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.3099\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.3045\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 37s 185ms/step - loss: 0.2991\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.2938\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.2883\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.2829\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.2775\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2720\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.2666\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2616\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.2567\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2517\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.2467\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2420\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.2375\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.2326\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.2280\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2235\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.2190\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.2147\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.2106\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.2063\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.2019\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1977\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.1938\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.1898\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.1862\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1824\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1786\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.1748\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1713\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1679\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.1645\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1612\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1580\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1547\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1515\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1481\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1451\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1421\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1391\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1362\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1334\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1310\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 36s 181ms/step - loss: 0.1285\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1258\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1233\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1208\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1185\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 36s 180ms/step - loss: 0.1161\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 39s 194ms/step - loss: 0.1136\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1112\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.1087\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 35s 175ms/step - loss: 0.1068\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 37s 184ms/step - loss: 0.1048\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 37s 186ms/step - loss: 0.1028\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.1004\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.0983\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.0964\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 36s 182ms/step - loss: 0.0945\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.0926\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.0909\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 35s 176ms/step - loss: 0.0892\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.0875\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 37s 183ms/step - loss: 0.0858\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.0839\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.0821\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.0805\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 35s 177ms/step - loss: 0.0787\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 35s 177ms/step - loss: 0.0771\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.0758\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.0745\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.0733\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 36s 178ms/step - loss: 0.0718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdce0281b80>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "steps_per_epoch = len(full_train_x) // batch_size\n",
    "\n",
    "generator = data_generator(full_train_x, full_train_y, full_train_y_RAW,\n",
    "                           batch_size, simpDefWordIndex, len(full_train_y[0]))\n",
    "model.fit(generator, epochs=100, batch_size=batch_size, steps_per_epoch=steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOneHotEncodingVector(simpDef, maxLengthOutput, wordIndex):\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(simpDef), maxLengthOutput, len(wordIndex) + 1), dtype='float32')\n",
    "    for i, word in enumerate(simpDef):\n",
    "        for j, token in enumerate(word):\n",
    "            index = wordIndex.get(token, 0)\n",
    "            if index > 0:\n",
    "                decoder_target_data[i, j-1, index] == 1.0\n",
    "    return decoder_target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simpDefWordIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at one time in the past; formerly.\\xa0'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y_RAW[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = full_train_y_RAW[0].split(' ')[0]\n",
    "print(t)\n",
    "simpDefWordIndex.get(t, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    [input_seq, output_seq[:, :-1]],\n",
    "    output_seq[:, 1:],\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 98) and (None, 98, 8992) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mModelCheckpoint(\n\u001b[1;32m      3\u001b[0m         filepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconvnet_from_scratch.keras\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m         monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      9\u001b[0m \u001b[39m#encoding_token, decoding_token, embedding_dim, latent_dim\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     14\u001b[0m     [train_x, train_y],\n\u001b[1;32m     15\u001b[0m     train_y,\n\u001b[1;32m     16\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     validation_data\u001b[39m=\u001b[39;49m([val_x, val_y],\n\u001b[1;32m     19\u001b[0m                      val_y)\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filec3kvxqe5.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1024, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n        return self.compiled_loss(\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 284, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/losses.py\", line 2004, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/tyler/miniconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py\", line 5532, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 98) and (None, 98, 8992) are incompatible\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "\n",
    "#encoding_token, decoding_token, embedding_dim, latent_dim\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [train_x, train_y],\n",
    "    train_y,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=([val_x, val_y],\n",
    "                     val_y)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  619  2287 15517  5533     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n",
      "['west african ceremonial trumpet']\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[[[2.0055654e-15 9.9998415e-01 7.5618428e-18 ... 2.2158636e-15\n",
      "   1.9407268e-15 1.8761797e-15]\n",
      "  [6.7705792e-09 8.6112636e-01 1.3016134e-07 ... 8.6268130e-09\n",
      "   7.1670230e-09 6.6186097e-09]\n",
      "  [3.0851634e-08 8.0451965e-03 9.9755089e-05 ... 3.7594070e-08\n",
      "   3.2784662e-08 3.0019322e-08]\n",
      "  ...\n",
      "  [4.5385038e-09 2.6561373e-05 3.5750124e-06 ... 4.6060724e-09\n",
      "   4.2870640e-09 4.8088720e-09]\n",
      "  [4.5473212e-09 2.6544154e-05 3.5513958e-06 ... 4.6153734e-09\n",
      "   4.2955155e-09 4.8179385e-09]\n",
      "  [4.5521897e-09 2.6488729e-05 3.5336695e-06 ... 4.6201913e-09\n",
      "   4.3003361e-09 4.8230331e-09]]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m newSentence \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([inputdata, outputdata])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(newSentence)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msimpDefTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_to_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewSentence\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/keras/preprocessing/text.py:419\u001b[0m, in \u001b[0;36mTokenizer.sequences_to_texts\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msequences_to_texts\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequences):\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;124;03m\"\"\"Transforms each sequence into a list of text.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m        A list of texts (strings)\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_to_texts_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.9/site-packages/keras/preprocessing/text.py:441\u001b[0m, in \u001b[0;36mTokenizer.sequences_to_texts_generator\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m    439\u001b[0m vect \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[0;32m--> 441\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_word\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m num_words \u001b[38;5;129;01mand\u001b[39;00m num \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_words:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data, tokens, tokenizer = convertTextToNumbers(\n",
    "    [\"West African ceremonial trumpet\"], defTokenizer, padding=len(train_x[0]))\n",
    "print(data)\n",
    "print(defTokenizer.sequences_to_texts(data))\n",
    "\n",
    "inputdata = np.array(data).reshape(1, -1)\n",
    "\n",
    "outputdata = np.zeros(shape=(1, len(train_y[0])))\n",
    "outputdata[0, 0] = simpDefTokenizer.word_index['<start>']\n",
    "#outputdata = np.reshape(outputdata, (1,1))\n",
    "\n",
    "\n",
    "newSentence = model.predict([inputdata, outputdata])\n",
    "print(newSentence)\n",
    "print(simpDefTokenizer.sequences_to_texts(newSentence))\n",
    "\n",
    "# Assuming you have already preprocessed and tokenized the sentence\n",
    "#test_sentence = \"the cat jumped over the moon\"\n",
    "#test_sentence_tokens = tokenizer.texts_to_sequences([test_sentence])[0]\n",
    "#test_sentence_tokens = np.array(test_sentence_tokens).reshape(1, -1)\n",
    "\n",
    "# Initialize the decoder input with the start token\n",
    "#decoder_input = np.zeros(shape=(1, max_summary_len))\n",
    "#decoder_input[0, 0] = summary_tokenizer.word_index['start']\n",
    "\n",
    "# Reshape the decoder input to have a length of 1\n",
    "#decoder_input = np.reshape(decoder_input, (1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting/Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39;49mhistory[\u001b[39m\"\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      3\u001b[0m val_accuracy \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a675ecb475794e98cc508edff942b4df7efb840dcd0161848ec7416e01ca0315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
