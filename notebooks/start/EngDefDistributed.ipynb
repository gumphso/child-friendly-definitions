{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 15:24:29.378135: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 15:24:30.071697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tensor/anaconda3/envs/tf/lib/:/home/tensor/anaconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-04-09 15:24:30.071774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/tensor/anaconda3/envs/tf/lib/:/home/tensor/anaconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-04-09 15:24:30.071780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.AUTO\n",
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf; \n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4911,
     "status": "ok",
     "timestamp": 1680126513113,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "KKThGm-729c9"
   },
   "outputs": [],
   "source": [
    "#pip install -q -U tensorflow-text==2.11.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_KWwOII29c-"
   },
   "source": [
    "Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1777,
     "status": "ok",
     "timestamp": 1680126518565,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "zpwNAGET4Ey5"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "#from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "\n",
    "def add_start_end(ragged, START, END):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  starts = tf.fill([count,1], START) # START\n",
    "  ends = tf.fill([count,1], END) # END\n",
    "  return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  # Drop the reserved tokens, except for \"[UNK]\".\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "  # Join them into strings.\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "\n",
    "    START = tf.argmax(tf.constant(self._reserved_tokens) == \"[start]\")\n",
    "    END = tf.argmax(tf.constant(self._reserved_tokens) == \"[end]\")\n",
    "\n",
    "    enc = add_start_end(enc, START, END)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1680126518567,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "jRhFIVla29dC"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "def createSimplifiedSentence(sentence):\n",
    "    #copySentence = \"[start] \" + sentence + \" [end]\"\n",
    "    return sentence.encode(encoding = 'UTF-8', errors = 'strict')\n",
    "\n",
    "\n",
    "\n",
    "def convertText(words):\n",
    "\n",
    "    for i, sentence in enumerate(words):\n",
    "      modifiedSentence = createSimplifiedSentence(sentence=sentence)\n",
    "      words[i] = modifiedSentence\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def createTokenizerVocab(defDS, simpDS):\n",
    "  bert_tokenizer_params=dict(lower_case=True, preserve_unused_token=True)\n",
    "  reserved_tokens=[\"[pad]\", \"[unk]\", \"[start]\", \"[end]\"]\n",
    "\n",
    "  bert_vocab_args = dict(\n",
    "      # The target vocabulary size\n",
    "      vocab_size = 30000,\n",
    "      # Reserved tokens that must be included in the vocabulary\n",
    "      reserved_tokens=reserved_tokens,\n",
    "      # Arguments for `text.BertTokenizer`\n",
    "      bert_tokenizer_params=bert_tokenizer_params,\n",
    "      # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "      learn_params={},\n",
    "  )\n",
    "\n",
    "  combined = np.concatenate((defDS, simpDS))\n",
    "  combinedDS = tf.data.Dataset.from_tensor_slices(combined)\n",
    "\n",
    "\n",
    "  en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    combinedDS.batch(1000).prefetch(tf.data.AUTOTUNE),\n",
    "    **bert_vocab_args\n",
    "  )\n",
    "\n",
    "  write_vocab_file('ENG.txt', en_vocab)\n",
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3534,
     "status": "ok",
     "timestamp": 1680126522089,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "RSBiOH3l3H6S",
    "outputId": "85bba3cf-e90b-44db-f083-94cd9f1b5668"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 66011,
     "status": "ok",
     "timestamp": 1680126588097,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "URsWI56E29dE"
   },
   "outputs": [],
   "source": [
    "fullDs = pd.read_csv(\"/home/tensor/Documents/Github/child-friendly-definitions/notebooks/start/fullDataset.csv\")\n",
    "fullDs = fullDs.sample(frac=1)  # shuffle the dataset\n",
    "fullDs = fullDs.astype(str)\n",
    "  \n",
    "x = fullDs['definition'].values[:10000]\n",
    "y = fullDs['simplified_definition'].values[:10000]\n",
    "\n",
    "val_x = fullDs['definition'].values[10001:10801]\n",
    "val_y = fullDs['simplified_definition'].values[10001:10801]\n",
    "# the two arrays below are tokenized and padded for thetf.data.Dataset.from_tensor_slices((full_train_x,full_train_y)) algorithm (possible to combine the two tokenizers into 1)\n",
    "full_train_x = convertText(x)\n",
    "full_train_y = convertText(y)\n",
    "\n",
    "full_val_x = convertText(val_x)\n",
    "full_val_y = convertText(val_y)\n",
    "\n",
    "createTokenizerVocab(full_train_x, full_train_y)\n",
    "# prepare the training data in a suitable format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1680126588100,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "xHAdlIvj--W5",
    "outputId": "7e189f64-2ebc-461d-e09c-e3c370197933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(len(y))\n",
    "print(len(val_x))\n",
    "print(len(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3889,
     "status": "ok",
     "timestamp": 1680126591972,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "CFqU6xKH5iXG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: EnglishTokenizer/assets\n"
     ]
    }
   ],
   "source": [
    "tokenizers = tf.Module()\n",
    "rTokens=[\"[pad]\", \"[unk]\", \"[start]\", \"[end]\"]\n",
    "tokenizers.en = CustomTokenizer(rTokens, 'ENG.txt')\n",
    "model_name = 'EnglishTokenizer'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1680126591973,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "NHg5v25X8sWd",
    "outputId": "4fa093ce-2064-446b-cbee-4e4a7a159257"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4074>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tf.saved_model.load(model_name)\n",
    "tokenizer.en.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1680126591973,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "vIqjODXSIwC8",
    "outputId": "9bf6438d-0b13-46f5-ef79-e8b50fe0a52b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'to give particular attention to; stress . '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1680126591974,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "_Q4M-vYeVvcA",
    "outputId": "505eed53-aa3b-4516-c4ba-489ddb08ce4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 46, 151, 187, 544, 46, 12, 2617, 830, 10, 3]]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.en.tokenize([full_train_y[0]])\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1680126591975,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "stQZh9kzB6LY",
    "outputId": "901e5cd6-d38e-4626-a188-8eb6b2463c59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[start]', b'to', b'give', b'particular', b'attention', b'to', b';',\n",
       "  b'st', b'##ress', b'.', b'[end]']]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.en.lookup(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1680126591976,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "9rVnfaVQWbSY",
    "outputId": "8a9d685e-53fa-4626-806a-7d47d0e9d95e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'to give particular attention to ; stress .'], dtype=object)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = tokenizer.en.detokenize(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1680126591976,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "aN1D9iU39cwD",
    "outputId": "7a8b9206-0220-4af4-bcf7-5dbc5df172c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 105, 32, 1500, 815, 1106, 3]]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tokenizer.en.tokenize([\"some random sentence\"])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1680126591976,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "3oUuegMA9cyv",
    "outputId": "0379ec64-e9be-40fa-aa1a-933b532e7393"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[start]', b'some', b'r', b'##and', b'##om', b'sentence', b'[end]']]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.en.lookup(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1680126591977,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "WFgiIERj9c1h",
    "outputId": "ff4519f2-64d5-44a3-9430-f973f293065e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'some random sentence'], dtype=object)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.en.detokenize(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1680126591977,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "R7cwgJnDWDZw",
    "outputId": "4e722673-f76a-41fb-e917-5c969f72ea90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((full_train_x,full_train_y))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1680126591978,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "zSAbopbKB3Ts",
    "outputId": "3b171238-7dc6-4df1-e354-98ffe67f6128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices((full_val_x, full_val_y))\n",
    "val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIxEajedSNZJ"
   },
   "source": [
    "Testing dataset for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1680126591980,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "tZ6Y-I89HuTN"
   },
   "outputs": [],
   "source": [
    "MAX_TOKENS=200\n",
    "def prepare_batch(DEF, SIMPDEF):\n",
    "    DEF = tokenizers.en.tokenize(DEF)      # Output is ragged.\n",
    "    DEF = DEF[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
    "    DEF = DEF.to_tensor()  # Convert to 0-padded dense Tensor\n",
    "\n",
    "    SIMPDEF = tokenizers.en.tokenize(SIMPDEF)\n",
    "    SIMPDEF = SIMPDEF[:, :(MAX_TOKENS+1)]\n",
    "    SIMPDEF_inputs = SIMPDEF[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    SIMPDEF_labels = SIMPDEF[:, 1:].to_tensor()   # Drop the [START] tokens\n",
    "\n",
    "    return (DEF, SIMPDEF_inputs), SIMPDEF_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1680126591981,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "1eByQv-xHzr-",
    "outputId": "98282bb4-a61b-484f-ec30-f651d65af0ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None)), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "  \n",
    "batches = make_batches(ds) # test\n",
    "batches # if it prints it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1680126591981,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "Q1-ScAy8BJeE"
   },
   "outputs": [],
   "source": [
    "val_batches = make_batches(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 901,
     "status": "ok",
     "timestamp": 1680126592863,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "Mv0UizXHSQjF",
    "outputId": "ce90b57a-4a77-4ea9-8201-e4e926ec5d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 76)\n",
      "(16, 44)\n",
      "(16, 44)\n"
     ]
    }
   ],
   "source": [
    "for (DEF, SIMP), SIMP_labels in batches.take(1):\n",
    "  break\n",
    "\n",
    "print(DEF.shape)\n",
    "print(SIMP.shape)\n",
    "print(SIMP_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkeqZx0j29dF"
   },
   "source": [
    "Defining Components of Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgi1-zTF29dG"
   },
   "source": [
    "Input/Output embedding + Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1680126592863,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "GFo8JjGq29dI"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "  \n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1680126592864,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "B78Rz7mQ29dI"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1680126592865,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "H-fYgpbgRx32",
    "outputId": "e5fe6d32-71ea-444e-a5bb-7b1288747197"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 44), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_DEF = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n",
    "embed_SIMP = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n",
    "\n",
    "DEF_emb = embed_DEF(DEF)\n",
    "SIMP_emb = embed_SIMP(SIMP)\n",
    "SIMP_emb._keras_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_ZX8rDV29dJ"
   },
   "source": [
    "BaseAttention Layer (Multi-Head Attention + add & Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1680126592866,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "kgdBAlwx29dJ"
   },
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOIFujHm29dJ"
   },
   "source": [
    "Cross Attention Layer (Base Attention Layer where the encoder meets the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1680126592866,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "nzE9Bdbw29dJ"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "   \n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1680126593825,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "fK9XFsLrUXOY",
    "outputId": "b11504ac-219d-44bd-afdf-8c69a563c14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 76, 512)\n",
      "(16, 44, 512)\n",
      "(16, 44, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(DEF_emb.shape)\n",
    "print(SIMP_emb.shape)\n",
    "print(sample_ca(SIMP_emb, DEF_emb).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-oF9LNg29dJ"
   },
   "source": [
    "Global Self Attention (input attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1680126593826,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "wI0MGu4R29dK"
   },
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1680126593826,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "68EqZUZLUg9t",
    "outputId": "48d2c9c1-fff1-41d4-bc89-779064fa9950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 76, 512)\n",
      "(16, 76, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(DEF_emb.shape)\n",
    "print(sample_gsa(DEF_emb).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l3lEEP429dK"
   },
   "source": [
    "Casual Attention Layer (Output attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1680126593827,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "C38ipdEV29dL"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1680126593827,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "iuorf__RUrng",
    "outputId": "cd9d4cd0-3517-4357-c570-44c277669c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 44, 512)\n",
      "(16, 44, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
    "\n",
    "print(SIMP_emb.shape)\n",
    "print(sample_csa(SIMP_emb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1680126593828,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "VFdXlcnqUxjd",
    "outputId": "098dc891-5685-4220-8ae6-232e8e8d50b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7683716e-07"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = sample_csa(embed_SIMP(SIMP[:, :3])) \n",
    "out2 = sample_csa(embed_SIMP(SIMP))[:, :3]\n",
    "\n",
    "tf.reduce_max(abs(out1 - out2)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kmVCej729dM"
   },
   "source": [
    "Feed Forward Layer (2 Dense and an add & Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1680126593828,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "7nCuo91P29dM"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1680126593829,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "b8PsCF92U51y",
    "outputId": "5620d293-d16b-4549-f52a-d5a1e42bc652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 44, 512)\n",
      "(16, 44, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_ffn = FeedForward(512, 2048)\n",
    "\n",
    "print(SIMP_emb.shape)\n",
    "print(sample_ffn(SIMP_emb).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6twMbGkD29dM"
   },
   "source": [
    "Encoder (Not including the input embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1680126593829,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "5M6jHqvL29dM"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1680126593830,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "vF623CtJVBXD",
    "outputId": "19a24a72-a2c4-41ee-8477-53f116756155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 44, 512)\n",
      "(16, 76, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "print(SIMP_emb.shape)\n",
    "print(sample_encoder_layer(DEF_emb).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7dcsQ1e29dN"
   },
   "source": [
    "Full Encoder Including Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1680126593830,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "J8rMiJ5_29dN"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 518,
     "status": "ok",
     "timestamp": 1680126594337,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "i7FB6VgjVV1q",
    "outputId": "9673bb2a-945c-432f-d4f1-23de95bc0cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 76)\n",
      "(16, 76, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the encoder.\n",
    "sample_encoder = Encoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8500)\n",
    "\n",
    "sample_encoder_output = sample_encoder(DEF, training=False)\n",
    "\n",
    "# Print the shape.\n",
    "print(DEF.shape)\n",
    "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvcLlII329dN"
   },
   "source": [
    "Decoder Without Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1680126594338,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "wdzGagd729dO"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "    \n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1680126594340,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "C-0Ig_pJVlMT",
    "outputId": "786ea256-8d48-420e-b9a3-5ef1de5a8b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 44, 512)\n",
      "(16, 76, 512)\n",
      "(16, 44, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
    "\n",
    "sample_decoder_layer_output = sample_decoder_layer(\n",
    "    x=SIMP_emb, context=DEF_emb)\n",
    "\n",
    "print(SIMP_emb.shape)\n",
    "print(DEF_emb.shape)\n",
    "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01eYtq7-29dO"
   },
   "source": [
    "Decoder With Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1680126594341,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "AIBvr6U629dO"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1111,
     "status": "ok",
     "timestamp": 1680126595436,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "uXDoVMnRViDw",
    "outputId": "987b6fc2-1993-4054-8c56-eabe892e18cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 44)\n",
      "(16, 76, 512)\n",
      "(16, 44, 512)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder.\n",
    "sample_decoder = Decoder(num_layers=4,\n",
    "                         d_model=512,\n",
    "                         num_heads=8,\n",
    "                         dff=2048,\n",
    "                         vocab_size=8000)\n",
    "\n",
    "output = sample_decoder(\n",
    "    x=SIMP,\n",
    "    context=DEF_emb)\n",
    "\n",
    "# Print the shapes.\n",
    "print(SIMP.shape)\n",
    "print(DEF_emb.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ced9ZLC229dP"
   },
   "source": [
    "Full Transformer with all layers combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1680126595436,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "gH9ImtWh29dP"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CPCqJmlWCfC"
   },
   "source": [
    "Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1680126595437,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "XgJRCZAF3jkr"
   },
   "outputs": [],
   "source": [
    "# num_layers=6, d_model=512, and dff=2048\n",
    "num_layers = 8\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 10\n",
    "dropout_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1680126596178,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "I0O2D4Wr3ucj"
   },
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oq2qfRK3WXtE"
   },
   "source": [
    "Creating the optimizer and find the bets learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1680126596177,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "gDafGA1MKUwT"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "with strategy.scope():\n",
    "    learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8KfydVAWbxU"
   },
   "source": [
    "Creating the masked loss (removes any weights that the padding may have)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7E-aEw6WsuV"
   },
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1680126595437,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "tHTCQusj3mBk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 15:25:03.228882: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:5\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "INFO:tensorflow:Collective all_reduce tensors: 338 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce IndexedSlices: 2 all_reduces, num_devices =2, group_size = 2, implementation = CommunicationImplementation.AUTO\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 338 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce IndexedSlices: 2 all_reduces, num_devices =2, group_size = 2, implementation = CommunicationImplementation.AUTO\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "625/625 [==============================] - ETA: 0s - loss: 6.5881 - masked_accuracy: 0.1552"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 15:29:19.317787: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 800\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:6\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.AUTO, num_packs = 1\n",
      "625/625 [==============================] - 266s 310ms/step - loss: 6.5881 - masked_accuracy: 0.1552 - val_loss: 5.3135 - val_masked_accuracy: 0.2097\n",
      "Epoch 2/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 4.9680 - masked_accuracy: 0.2313 - val_loss: 4.6737 - val_masked_accuracy: 0.2563\n",
      "Epoch 3/40\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 4.5581 - masked_accuracy: 0.2605 - val_loss: 4.4754 - val_masked_accuracy: 0.2694\n",
      "Epoch 4/40\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 4.3883 - masked_accuracy: 0.2702 - val_loss: 4.3942 - val_masked_accuracy: 0.2745\n",
      "Epoch 5/40\n",
      "625/625 [==============================] - 169s 271ms/step - loss: 4.2833 - masked_accuracy: 0.2776 - val_loss: 4.3317 - val_masked_accuracy: 0.2803\n",
      "Epoch 6/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 4.2113 - masked_accuracy: 0.2800 - val_loss: 4.3263 - val_masked_accuracy: 0.2778\n",
      "Epoch 7/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 4.1629 - masked_accuracy: 0.2836 - val_loss: 4.2411 - val_masked_accuracy: 0.2858\n",
      "Epoch 8/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 4.0353 - masked_accuracy: 0.2932 - val_loss: 4.1562 - val_masked_accuracy: 0.2921\n",
      "Epoch 9/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 3.9079 - masked_accuracy: 0.3032 - val_loss: 4.0861 - val_masked_accuracy: 0.3006\n",
      "Epoch 10/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 3.7967 - masked_accuracy: 0.3128 - val_loss: 4.0458 - val_masked_accuracy: 0.3007\n",
      "Epoch 11/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 3.6867 - masked_accuracy: 0.3208 - val_loss: 3.9957 - val_masked_accuracy: 0.3061\n",
      "Epoch 12/40\n",
      "625/625 [==============================] - 166s 265ms/step - loss: 3.5895 - masked_accuracy: 0.3302 - val_loss: 3.9895 - val_masked_accuracy: 0.3098\n",
      "Epoch 13/40\n",
      "625/625 [==============================] - 167s 267ms/step - loss: 3.4903 - masked_accuracy: 0.3388 - val_loss: 3.9615 - val_masked_accuracy: 0.3121\n",
      "Epoch 14/40\n",
      "625/625 [==============================] - 166s 266ms/step - loss: 3.3999 - masked_accuracy: 0.3481 - val_loss: 3.9492 - val_masked_accuracy: 0.3144\n",
      "Epoch 15/40\n",
      "625/625 [==============================] - 166s 265ms/step - loss: 3.3217 - masked_accuracy: 0.3568 - val_loss: 3.9439 - val_masked_accuracy: 0.3154\n",
      "Epoch 16/40\n",
      "625/625 [==============================] - 168s 269ms/step - loss: 3.2392 - masked_accuracy: 0.3643 - val_loss: 3.9330 - val_masked_accuracy: 0.3154\n",
      "Epoch 17/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 3.1687 - masked_accuracy: 0.3720 - val_loss: 3.9491 - val_masked_accuracy: 0.3189\n",
      "Epoch 18/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 3.0937 - masked_accuracy: 0.3800 - val_loss: 3.9581 - val_masked_accuracy: 0.3188\n",
      "Epoch 19/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 3.0296 - masked_accuracy: 0.3882 - val_loss: 3.9717 - val_masked_accuracy: 0.3187\n",
      "Epoch 20/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 2.9618 - masked_accuracy: 0.3967 - val_loss: 3.9694 - val_masked_accuracy: 0.3173\n",
      "Epoch 21/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 2.8976 - masked_accuracy: 0.4049 - val_loss: 3.9989 - val_masked_accuracy: 0.3175\n",
      "Epoch 22/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 2.8462 - masked_accuracy: 0.4122 - val_loss: 4.0145 - val_masked_accuracy: 0.3212\n",
      "Epoch 23/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.7899 - masked_accuracy: 0.4185 - val_loss: 4.0228 - val_masked_accuracy: 0.3167\n",
      "Epoch 24/40\n",
      "625/625 [==============================] - 170s 271ms/step - loss: 2.7353 - masked_accuracy: 0.4270 - val_loss: 4.0306 - val_masked_accuracy: 0.3219\n",
      "Epoch 25/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.6860 - masked_accuracy: 0.4331 - val_loss: 4.0795 - val_masked_accuracy: 0.3200\n",
      "Epoch 26/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.6360 - masked_accuracy: 0.4404 - val_loss: 4.1155 - val_masked_accuracy: 0.3196\n",
      "Epoch 27/40\n",
      "625/625 [==============================] - 169s 271ms/step - loss: 2.5834 - masked_accuracy: 0.4490 - val_loss: 4.1248 - val_masked_accuracy: 0.3132\n",
      "Epoch 28/40\n",
      "625/625 [==============================] - 169s 271ms/step - loss: 2.5451 - masked_accuracy: 0.4541 - val_loss: 4.1517 - val_masked_accuracy: 0.3188\n",
      "Epoch 29/40\n",
      "625/625 [==============================] - 170s 271ms/step - loss: 2.5049 - masked_accuracy: 0.4585 - val_loss: 4.1782 - val_masked_accuracy: 0.3176\n",
      "Epoch 30/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.4595 - masked_accuracy: 0.4678 - val_loss: 4.1936 - val_masked_accuracy: 0.3140\n",
      "Epoch 31/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 2.4223 - masked_accuracy: 0.4726 - val_loss: 4.2021 - val_masked_accuracy: 0.3178\n",
      "Epoch 32/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.3815 - masked_accuracy: 0.4787 - val_loss: 4.2366 - val_masked_accuracy: 0.3195\n",
      "Epoch 33/40\n",
      "625/625 [==============================] - 169s 271ms/step - loss: 2.3452 - masked_accuracy: 0.4854 - val_loss: 4.2651 - val_masked_accuracy: 0.3190\n",
      "Epoch 34/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.3166 - masked_accuracy: 0.4907 - val_loss: 4.2767 - val_masked_accuracy: 0.3219\n",
      "Epoch 35/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.2762 - masked_accuracy: 0.4972 - val_loss: 4.3039 - val_masked_accuracy: 0.3143\n",
      "Epoch 36/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 2.2395 - masked_accuracy: 0.5036 - val_loss: 4.3277 - val_masked_accuracy: 0.3126\n",
      "Epoch 37/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.2092 - masked_accuracy: 0.5090 - val_loss: 4.3404 - val_masked_accuracy: 0.3138\n",
      "Epoch 38/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.1762 - masked_accuracy: 0.5137 - val_loss: 4.3847 - val_masked_accuracy: 0.3195\n",
      "Epoch 39/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.1484 - masked_accuracy: 0.5190 - val_loss: 4.4193 - val_masked_accuracy: 0.3132\n",
      "Epoch 40/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 2.1196 - masked_accuracy: 0.5238 - val_loss: 4.4074 - val_masked_accuracy: 0.3158\n"
     ]
    }
   ],
   "source": [
    "inputVocabSize = tokenizers.en.get_vocab_size().numpy()\n",
    "outputVocabSize = inputVocabSize\n",
    "#tf.placeholder\n",
    "with strategy.scope():\n",
    "    transformer = Transformer(\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        input_vocab_size=inputVocabSize, # this has been changed\n",
    "        target_vocab_size=outputVocabSize, # this has been changed\n",
    "        dropout_rate=dropout_rate)\n",
    "    transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"model.hs\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\")\n",
    "]\n",
    "    history = transformer.fit(batches,\n",
    "                epochs=40,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1680126596175,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "xLFrNt6rWLqr",
    "outputId": "db59768a-7128-4376-b398-07f436ed469d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 44)\n",
      "(16, 76)\n",
      "(16, 44, 4074)\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "output = transformer((DEF, SIMP))\n",
    "\n",
    "\n",
    "print(SIMP.shape)\n",
    "print(DEF.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3371179,
     "status": "ok",
     "timestamp": 1680129969779,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "5D3doeap4Xax",
    "outputId": "2f2b9a9c-c77a-4b12-e237-ae23398f874f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 17:20:07.072505: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 10000\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:5\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "625/625 [==============================] - ETA: 0s - loss: 2.0949 - masked_accuracy: 0.5294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 17:22:52.784243: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:784] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_2\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_STRING\n",
      "      type: DT_STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 800\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:6\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_STRING\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 170s 272ms/step - loss: 2.0949 - masked_accuracy: 0.5294 - val_loss: 4.4322 - val_masked_accuracy: 0.3134\n",
      "Epoch 2/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.0595 - masked_accuracy: 0.5358 - val_loss: 4.4568 - val_masked_accuracy: 0.3108\n",
      "Epoch 3/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 2.0318 - masked_accuracy: 0.5405 - val_loss: 4.4938 - val_masked_accuracy: 0.3120\n",
      "Epoch 4/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 2.0118 - masked_accuracy: 0.5433 - val_loss: 4.4684 - val_masked_accuracy: 0.3048\n",
      "Epoch 5/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.9877 - masked_accuracy: 0.5486 - val_loss: 4.4995 - val_masked_accuracy: 0.3204\n",
      "Epoch 6/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.9649 - masked_accuracy: 0.5540 - val_loss: 4.5179 - val_masked_accuracy: 0.3166\n",
      "Epoch 7/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 1.9444 - masked_accuracy: 0.5561 - val_loss: 4.5433 - val_masked_accuracy: 0.3144\n",
      "Epoch 8/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 1.9186 - masked_accuracy: 0.5611 - val_loss: 4.5390 - val_masked_accuracy: 0.3148\n",
      "Epoch 9/40\n",
      "625/625 [==============================] - 171s 274ms/step - loss: 1.8981 - masked_accuracy: 0.5660 - val_loss: 4.5928 - val_masked_accuracy: 0.3111\n",
      "Epoch 10/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 1.8811 - masked_accuracy: 0.5700 - val_loss: 4.5843 - val_masked_accuracy: 0.3149\n",
      "Epoch 11/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.8599 - masked_accuracy: 0.5754 - val_loss: 4.6101 - val_masked_accuracy: 0.3082\n",
      "Epoch 12/40\n",
      "625/625 [==============================] - 171s 274ms/step - loss: 1.8421 - masked_accuracy: 0.5783 - val_loss: 4.5904 - val_masked_accuracy: 0.3156\n",
      "Epoch 13/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.8262 - masked_accuracy: 0.5819 - val_loss: 4.6504 - val_masked_accuracy: 0.3085\n",
      "Epoch 14/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 1.8053 - masked_accuracy: 0.5858 - val_loss: 4.6369 - val_masked_accuracy: 0.3138\n",
      "Epoch 15/40\n",
      "625/625 [==============================] - 170s 273ms/step - loss: 1.7883 - masked_accuracy: 0.5888 - val_loss: 4.6276 - val_masked_accuracy: 0.3111\n",
      "Epoch 16/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.7681 - masked_accuracy: 0.5929 - val_loss: 4.6407 - val_masked_accuracy: 0.3200\n",
      "Epoch 17/40\n",
      "625/625 [==============================] - 169s 270ms/step - loss: 1.7562 - masked_accuracy: 0.5958 - val_loss: 4.6534 - val_masked_accuracy: 0.3126\n",
      "Epoch 18/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.7397 - masked_accuracy: 0.5992 - val_loss: 4.6597 - val_masked_accuracy: 0.3125\n",
      "Epoch 19/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 1.7252 - masked_accuracy: 0.6037 - val_loss: 4.6958 - val_masked_accuracy: 0.3150\n",
      "Epoch 20/40\n",
      "625/625 [==============================] - 172s 275ms/step - loss: 1.7081 - masked_accuracy: 0.6068 - val_loss: 4.7241 - val_masked_accuracy: 0.3166\n",
      "Epoch 21/40\n",
      "625/625 [==============================] - 170s 271ms/step - loss: 1.6936 - masked_accuracy: 0.6096 - val_loss: 4.7253 - val_masked_accuracy: 0.3086\n",
      "Epoch 22/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.6822 - masked_accuracy: 0.6117 - val_loss: 4.7432 - val_masked_accuracy: 0.3096\n",
      "Epoch 23/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.6682 - masked_accuracy: 0.6146 - val_loss: 4.7614 - val_masked_accuracy: 0.3095\n",
      "Epoch 24/40\n",
      "625/625 [==============================] - 171s 273ms/step - loss: 1.6501 - masked_accuracy: 0.6198 - val_loss: 4.7662 - val_masked_accuracy: 0.3114\n",
      "Epoch 25/40\n",
      "625/625 [==============================] - 170s 271ms/step - loss: 1.6433 - masked_accuracy: 0.6218 - val_loss: 4.7611 - val_masked_accuracy: 0.3103\n",
      "Epoch 26/40\n",
      "625/625 [==============================] - 169s 271ms/step - loss: 1.6252 - masked_accuracy: 0.6269 - val_loss: 4.7989 - val_masked_accuracy: 0.3115\n",
      "Epoch 27/40\n",
      "625/625 [==============================] - 170s 272ms/step - loss: 1.6197 - masked_accuracy: 0.6266 - val_loss: 4.8055 - val_masked_accuracy: 0.3151\n",
      "Epoch 28/40\n",
      "417/625 [===================>..........] - ETA: 53s - loss: 1.5617 - masked_accuracy: 0.6375"
     ]
    }
   ],
   "source": [
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"model.hs\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\")\n",
    "]\n",
    "    history = transformer.fit(batches,\n",
    "                epochs=40,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1680129969782,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "LnsQnXd_Et1h",
    "outputId": "667db9e1-3602-4aee-fbe3-2c74f07443dc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"loss\"]\n",
    "val_accuracy = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation\")\n",
    "plt.title(\"Training and validation LOSS\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1680129970362,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "dRD_Hl7xEt4W",
    "outputId": "96fb42ad-993e-4fc0-f71b-a14f3939c999"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"masked_accuracy\"]\n",
    "val_accuracy = history.history[\"val_masked_accuracy\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation\")\n",
    "plt.title(\"Training and validation masked_accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoW3TM-iZuoU"
   },
   "source": [
    "Predicting on New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1680129970364,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "1-7f0aa_ZwsB"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.en.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
    "\n",
    "    tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1680129970366,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "UrFtwWkoa7KP"
   },
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF-Y3VZUbNWC"
   },
   "source": [
    "Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1680129970367,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "IciTOXXPbN89"
   },
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    (result,\n",
    "     tokens,\n",
    "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1680129970368,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "0U9svpb7bSnO"
   },
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TV0bQnTvbXw0"
   },
   "source": [
    "Random Predictions (Hopefully Works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10477,
     "status": "ok",
     "timestamp": 1680129980824,
     "user": {
      "displayName": "Tyler Jarboe",
      "userId": "08629846309066612484"
     },
     "user_tz": 240
    },
    "id": "o3zr2RYybaY1",
    "outputId": "608d2df6-e056-44d5-85bc-9623c1069065"
   },
   "outputs": [],
   "source": [
    "translator('Some random assortment of words that no one understands.').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a675ecb475794e98cc508edff942b4df7efb840dcd0161848ec7416e01ca0315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
